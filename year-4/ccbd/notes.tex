% Preamble {{{
\documentclass[11pt,a4paper,titlepage,dvipsnames,cmyk]{scrartcl}
\usepackage[english]{babel}
\typearea{12}
% }}}

% Set indentation and line skip for paragraph {{{
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}
\usepackage[margin=2cm]{geometry}
\addtolength{\textheight}{-1in}
\setlength{\headsep}{.5in}
% }}}

\usepackage{hhline}
\usepackage[table]{xcolor}
\usepackage{mathtools}
\usepackage{tabularx}
\usepackage[T1]{fontenc}

% Headers setup {{{
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Cloud Computing and Big Data}
\rhead{Josh Felmeden}
\usepackage{hyperref}
% }}}

% Listings {{{
\usepackage[]{listings}
\lstset
{
    breaklines=true,
    tabsize=3,
    showstringspaces=false
}

\definecolor{lstgrey}{rgb}{0.05,0.05,0.05}
\usepackage{listings}
\makeatletter
\lstset{language=[Visual]Basic,
backgroundcolor=\color{lstgrey},
frame=single,
xleftmargin=0.7cm,
frame=tlbr, framesep=0.2cm, framerule=0pt,
basicstyle=\lst@ifdisplaystyle\color{white}\footnotesize\ttfamily\else\color{black}\footnotesize\ttfamily\fi,
captionpos=b,
tabsize=2,
keywordstyle=\color{Magenta}\bfseries,
identifierstyle=\color{Cyan},
stringstyle=\color{Yellow},
commentstyle=\color{Gray}\itshape
}
\makeatother
\renewcommand{\familydefault}{\sfdefault}
\newcommand{\specialcell}[2][c]{%
\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}
% }}}

% Other packages {{{
\usepackage{graphicx}
\graphicspath{ {./pics/} }
\usepackage{needspace}
\usepackage{tcolorbox}
\usepackage{soul}
\usepackage{babel,dejavu,helvet}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{tcolorbox}
\usepackage[symbol]{footmisc}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{enumitem}
\setlist{nolistsep}
% }}}

% tcolorbox {{{
\newtcolorbox{blue}[3][] {
colframe = #2!25,
colback = #2!10,
#1,
}

\newtcolorbox{titlebox}[3][] {
colframe = #2!25,
colback = #2!10,
coltitle = #2!20!black,
title = {#3},
fonttitle=\bfseries
#1,
}
% }}}

% Title {{{
\title{Cloud Computing and Big Data}
\author{Josh Felmeden}
% }}}

\begin{document}
\maketitle
\tableofcontents
\newpage
\section{Overview}
\subsection{Economic Driving Factors}
Cloud computing works by charging someone to use a service for a certain amount of time. If, for example, you let someone use your computer, you might charge them for their usage (if you were a real meanie). How that would be worked out is the \textit{operation expenditure} (Opex) and the \textit{capital expenditure} (Capex, cost of the computer). Combining these, we calculate the \textit{total cost of operation} and calculate the cost per day of the lifespan of the computer (in this example). So, if you used the computer for an hour, you would owe the person a 24th of this daily cost.

Now, this might come down to half a penny, but of course, this doesn't exist any more, so we might charge a whole penny instead. This seems like a marginal profit, but in terms of percentages, this is a $100\%$ increase. The fundamentals of this are how cloud computing generates so much income.

The attraction of this is that the users of the services do not have to pay the Capex, and simply pay opex for the rental of the service.

\subsection{Normal Failure}
Failures in these systems are to be expected. Say the servers you are buying are guaranteed to have a $99.999\%$ 3-year survival rate (`five-nines reliability'). This is good because there is a very high chance that this remains. Now, if you buy 10 of these servers, the probability that you have all of the servers still working is only $99.99\%$. Taking this to the extremes, if you buy 500,000 servers (this is standard practise for a lot of the big servers these days), the probability that all of them are still working is a measly $1\%$. Essentially, failure is something that should be normalised.

\textit{Modular data centres} are used a lot in big data centres. A unit may be left in the shipping container, and this container is removed or added as a container, meaning that if one module fails, another can just be replaced, meaning that the whole centre doesn't collapse.

In the current climate, the modular centres are considered a whole working unit, and this way of thinking was mobilised by a group of Google engineers.

\subsection{Blank as a service}
\begin{itemize}
    \item \textbf{SaaS: Software as a service}
    \begin{itemize}
        \item End user application software that is remotely delivered over the internet.
        \item Adobe is an example of this (Adobe Creative Cloud)
        \item Used to be bought off the shelf as a CD
    \end{itemize}
    \item \textbf{PaaS: Platform as a Service}
    \begin{itemize}
        \item Developer application software (middleware) functionality is remotely accessible
        \item Might provide a particular combination of OS, web-server, data-base and scripting
        \item Popular instance is the free `LAMP stack' (Linux, Apache, MYSQL and PHP)
        \item Used to be dominated by \textit{Google}
    \end{itemize}
    \item \textbf{IaaS: Infrastructure as a Service}
    \begin{itemize}
        \item IT infrastructure almost always virtualised and remotely accessible.
        \item Virtualisation software allows one physical server to be used by multiple users, each on a virtual machine. If one crashes, the others keep running.
        \item Used to be dominated by \textit{AWS} (although this is now a much bigger thing).
    \end{itemize}
\end{itemize}

While Google and Amazon dominated their respective fields, both of these companies have expanded into the other fields. This is very complicated, but the key thing is that both Google and Amazon offer both PaaS and IaaS.

In around 2015, Amazon created \textbf{FaaS: function as a service (aka `serverless')}:
\begin{itemize}
    \item No server processes visibly running. Pay only for the time spent executing a function
    \item UNlike PaaS, scale out without increasing number of servers.
    \item Amazon Lambda is the best known example, although both Google and Microsoft have answers to this.
\end{itemize}

\subsection{Impacts of Cloud Services}
Cloud services have revolutionised business in many ways. It is now possible to do tasks that would previously require access to high performance machines. Instead, it can be sent to a big data centre, and this usage is just charged as rental.

\textit{Interoperability} is a big issue. \textbf{Vendor lock-in} is a concern for a lot of clients, and this simply means that once a client is locked into a vendor, it becomes financially or practically unviable to switch to another supplier. This led to an attempt to develop a sense of unity between cloud companies, where companies created the \textit{Open Cloud Manifesto}. A lot of big companies signed up to this, but a lot of the top dogs didn't sign up for this (unsurprisingly, Google and Amazon). This manifesto seemingly no longer exists as an original. As a consumer, this is bad and worrying because vendor lock in is very possible.

\section{IaaS and AWS}
\subsection{Amazon Simple Storage Service}
Amazon S3 is a cloud-based persistent storage. It operates independently from other Amazon services. The simple refers to the features, not that it's simple to use. You can store data in the cloud. You also don't store files, you store \textit{objects}, and these are kept in buckets. Objects have a size limit (5Tb) and a max size on a single upload is 5Gb. All buckets share the same namespace, so no sub-buckets.

It's very easy to use; just use a web GUI that is similar to AWS. It also has a command line interface and has scripting. Default storage can be selected (geographically).

S3 is accessed via API, either by SOAP (xml) or REST (http). Wrappers are available to abstract the API for programmers.

This is just the storage, so now we will look at the computing of data in the cloud.

\subsection{Amazon Elastic Compute Cloud (EC2)}
This is a remotely accessible virtual network of virtual servers. Usually, EC2 is run with S3 providing the storage.

A single EC2 virtual server, with the chosen OS etc, is an instance. An instance is instantiatied from an Amazon Machine Image (AMI)
\begin{itemize}
    \item One AMI can be cloned $n$ times to create $n$ instances
    \item You can build your own by cloning an AMI from your local server
    \item Or, Amazon have a bunch of prebuilt AMIs that you can choose from
\end{itemize}

EC2 dynamically assigns a unique IP address to each instance, and this IP can be reassigned, perhaps to someone else. The IP can also be static (also known as an Elastic IP address) at a cost.

EC2 instances run in availability zones (AZs), grouped into regions. AZ is similar to a single data centre, guaranteeing an area has 99.95\% uptime.

\subsubsection{Usage}
Some basic API routes as S3, command-line or a bit of GUI too:
\begin{itemize}
    \item Amazon's own AWS web console
    \item Various EC2 plug-ins for browsers
    \item Third-party cloud management tools
\end{itemize}

Some AMIs are junk or malware, however, so be careful when selecting this.

There are three types of storage:
\begin{itemize}
    \item Ephemeral local storage in the instance (dies with instance)
    \item Persistent cloud (S3)
    \item SAN-style Elastic Block Storage (EBS)
    \begin{itemize}
        \item Allows user to create volumes from 1Gb to 1Tb
        \item Any number of volumes may be mounted from a single instance
    \end{itemize}
\end{itemize}

S3 is slow, medium-reliable, but super-durable. Never loses data, so is good for DR backups. Instance storage is simple and cheap, but speed can be really poor. EBS is high on everything, but is complex and costly.

There is some autoscaling based on matrics:
\begin{itemize}
   \item \textbf{Cloudwatch}: automated monitoring of EC2 instances. Reveals many statistics such as CPU utilisation, disk reads/writes and network traffic. Aggregates and stores monitoring data that can be accessed
   \item \textbf{Auto Scaling}: dynamically adds or removes EC2 instances based on CloudWatch metrics. You define conditions upon which you want to scale up or down your EC2 instances. Auto scaling automatically adds or removes the specified amount of Amazon EC2 instances when it detects that the conditions have been met.
   \item \textbf{Elastic Load Balancing}: automatically distributes incoming application traffic across multiple EC2 instances. Better fault tolerance. Elastic Load Balancing detects unhealthy instances within a pool and automatically reroutes traffic to healthy instances until the unhealthy instances have been restored. Customers can enable ELB within a single AZ or across multiple for consistent application performance.
\end{itemize}

\subsection{AWS Simple Queue Service SQS and Architecting for Scale-out}
SQS is reliable, loosely-coupled fault-tolerant storage and delivery of messages. It can be between any clients or computers connected to the internet, and senders and recipients do not have to communicate directly. No requirement that either side be always-available or connected to the internet.

A \textbf{message} is up to 256Kb of text-data, sent to SQS and stored until it is delivered. A \textbf{queue} serves to group related messages together.

SQS is accessible to clients on any HTTP-enabled platform. Messages are stored redundantly over multiple data-centres truly distributed. Unfortunately, this brings about a few down-sides:
\begin{itemize}
    \item Message retrievals may be incomplete
    \item Messages may not be delivered quickly (2-10 seconds)
    \item Messages may be delivered out of order
    \item Messages may be redelivered
\end{itemize}

\subsubsection{Mitch Garnaat's Monster Muck Mashup}
This is a service that converts AVI videos to mp4 using:
\begin{itemize}
    \item `Boto' Python interface to AWS
    \item S3 to store the video files
    \item EC2 to do the conversion processing
    \item SQS for inter-process communication
\end{itemize}

Uses AWS for \textit{scalability} (scale-out not up, not just buying new resources, adding cheap machines to improve ability.)

The basic steps are:
\begin{enumerate}
\item Upload a bunch of video files to a S3 bucket
\item For each file, add a msg to SQS input queue
\item On the EC2 instance, repeat this until input queue is empty:
\begin{enumerate}
    \item Read message MI from input queue
    \item Retrieve from S3 the video VI specified in MI
    \item Do the conversion creating VO
    \item Store VO in S3
    \item Write message MO to SQS output queue
    \item Delete MI from input queue
\end{enumerate}
\end{enumerate}

This really illustrates how good AWS is at scaling software. It is good because any number of clients can connect to the bucket. If this bucket is 100\% full, it doesn't matter, because additional instances can all talk to the same buckets and queues, so the workload is met.

\subsection{AWS simpleDB and AWS Relational Databases (RDB)}
\subsubsection{Amazon SimpleDB}
This software provides:
\begin{itemize}
    \item Reliable storage of structured textual data
    \item Languages that allows you to store, modify, query, and retrieve data sets
    \item Automatic indexing of all stored data
\end{itemize}

SimpleDB provides 3 main resources:
\begin{itemize}
    \item \textbf{Domains}: Highest-level container for related data \textit{items}: queries only search within one domain
    \item \textbf{Items}: a named collection of \textit{attributes} that represent a data object. Each item has a unique name within the domain; items can be created, modified, or deleted; individual attributes within an an item can be manipulated
    \item \textbf{Attributes}: an individual category of information within the item, with a name unique for that item. Item has one or more text string values associated with the name
\end{itemize}

The downside to this type of storage is that it really does only do one data type (textual). If, for example, you wanted to store pi, you would need to store it as a character string (`3', `.', `1').

There is no Database \textbf{schema}, meaning if you mistype something, the database will just accept it as a definition, leading to some unfortunate results. It is not a traditional relational database management system:
\begin{itemize}
    \item SimpleDB items are stored in a hierarchical structure, not a table
    \item SimpleDB attribute value max size is 1Kb
    \item SimpleDB data is all stored as text
    \item The query language is really basic
    \item SimpleDB is distributed, so data consistency may suffer due to propagation delays
\end{itemize}

\subsubsection{Amazon Relation Database Service}
There is also a relational database system, possibly as a response to Azure. You can set up, operate and scale a full MySQL RDBMS without having to worry about infrastructure provisioning, software maintenance, or common DB management tasks, like backups.

The processing power and storage space can be scaled as needed with a single API call and you can fully initiate fully consistent database snapshots at any time. Can import a dump file to get started. Each DB instance exports a number of metrics to CloudWatch including CPU utilisation, etc.

\subsection{Availability Zones}
Availability zones are clusters of independent data centres that are up to 20 miles apart. They are interconnected using low latency and enable fault isolation and HA.

Choosing which region to use comes down to a few reasons:
\begin{itemize}
    \item Data sovereignty and compliance
    \begin{itemize}
        \item Where are you storing user data?
    \end{itemize}
    \item Proximity of users to data
    \item Services and feature availability
    \item Cost effectiveness
    \begin{itemize}
        \item Each region has differing costs
    \end{itemize}
\end{itemize}

High Availability (HA) is the ability to minimise service downtime by using redundant components. It also requires service components in at least two AZs.

Fault tolerance is the same as HA, but also the ability to ensure that no service disruption by using active-active architecture meaning that all components are active all the time. This is of course a lot more costly that just having HA.

IaaS may have HA, but FT unlikely. PaaS will usually have HA, but some services offer FT.

\section{Virtualisation}
The most fundamental type of cloud computing is IaaS compute, and the most fundamental type of this is the virtual machine. It has unmanaged services, which means you control what they do. You create, save or reuse them. They are networked and connected to storage and have certain security systems.

EC2 instances are a form of virtualisation.
\begin{itemize}
    \item They mostly run on Xeon processors, but also have other processors available.
    \item There are lots of different tiers available on AWS that use different purposes
    \item They run in AMIs.
    \item After creating a virtual machine, you can start, stop, and terminate the machines. Once the machine is terminated, it will not come back.
\end{itemize}

In virtualisation, there is some virtual memory that points to addresses in physical memory. It is an abstraction of the storage resources, because the virtual memory seems contiguous, but in physical memory it could be in various locations. The operating system manages this and also has hardware support.

\begin{center}
    \begin{tabularx}{\textwidth}{X|X}
        \textbf{Virtual Memory} & \textbf{Virtual Machine} \\ \midrule
        Abstraction of the RAM memory resources & Abstraction of the storage/process/IO resources. \\
        Mapping of program (virtual) memory addresses to physical addresses & Time slicing VM use of virtual memory addresses/CPU/IO registers in physical addresses. \\
        Operating system manages & Operating system/Hypervisor manages \\
        Hardware support (memory management unit) & Hardware support (e.g. Intel VT)
    \end{tabularx}
\end{center}

\subsection{The hypervisor}


This virtualisation is not a new idea. It has been in use since around 1960, but it has been formalised in 1974, where they defined three important properties:
\begin{itemize}
    \item \textit{Fidelity}: Program gets the same output whether on VM or hardware
    \item \textit{Performance}: Performs close to physical computer
    \item \textit{Safety}: Cannot change or corrupt data on physical computer
\end{itemize}

This was ensured by analysing the instruction sets and identified two instructions that are the most important: \textbf{sensitive} instructions (that can change configurations of resources) and \textbf{privileged} instructions. Sensitive instructions need to be caught by the operating system, jumping from user mode to kernel mode. Therefore, all sensitive instructions need to be a subset of privileged instructions. This is all handled by the hypervisor.

Therefore, we have the hypervisor in contact with the server hardware, and then on top of this, we have the VM (or multiple machines) that consist of a guest OS, middleware, and apps. This type of VM is called \textbf{bare metal}.

Another type, called \textbf{hosted}, has a host OS running on top of the server hardware, and then the hypervisor.

The difference between the two is clear; in type two, the hypervisor can write to the host OS meaning that it can be more lightweight, where as type one can be much faster, at the cost of having to contain stuff that is normally handled by the OS.

\subsubsection{Simulated Hardware}
\textit{Full virtualisation} is a complete or almost complete simulation of the underlying guest-machine hardware: virtualised guest OS runs as if it were on a bare machine.

The alternative to this is \textit{paravirtualisation} and is only possible when the source code of the OS is available. The guest OS is edited and recompiled to make system calls into the hypervisor API to execute safe rewrites of sensitive instructions. In this example, the hypervisor doesn't simulate hardware.

EC2 offers both of these virtual machines:
\begin{itemize}
    \item HVM --- hardware virtual machine
    \begin{itemize}
        \item Virtualised set of hardware
        \item Can use OS without modification
        \item Intel virtualisation technology
    \end{itemize}
    \item PV --- Paravirtualisation
    \begin{itemize}
        \item Requires OS to be prepared
        \item Doesn't support GPU instances
    \end{itemize}
\end{itemize}

\subsubsection{Xen}
Xen is a free, open-source hypervisor developed at University of Cambridge. It has multiple modes:
\begin{itemize}
    \item Paravirtualisation: guest OS recompiled with modifications
    \item Hardware assisted virtualisation: Intel x86 and ARM extensions
    \item Widespread
    \item Not easily virtualisable (17 instructions that violate the sensitive rules above)
\end{itemize}

\subsubsection{Areas of Virtualisation}
There are three main areas of responsibility for virtualisation managers:
\begin{itemize}
    \item CPU virtualisation
    \begin{itemize}
        \item Guest has exclusive use of a CPU for a period of time
        \item CPU state of the first guest is saved, and the state of the next guest is loaded before control is passed to it
    \end{itemize}
    \item Memory virtualisation
    \begin{itemize}
        \item Additional layer of indirection to virtual memory
    \end{itemize}
    \item I/O virtualisation
    \begin{itemize}
        \item Hypervisor implements a device model to provide abstractions of the hardware
    \end{itemize}
\end{itemize}

\subsubsection{KVM}
Converts Linux into a type-1 hypervisor. Memory manager, process scheduler, I/O stack from Linux. The VM is implemented as a regular Linux process. KVM requires CPU virtualisation extensions to handle instructions.

\subsubsection{Nitro Hypervisor}
\begin{itemize}
    \item Based on KVM
    \item Offloads virtualisation functions to dedicated hardware and software
    \item Hypervisor mainly provides CPU and memory isolation to EC2 instances
    \item Nitro cards for VPC networking and EBS storage
    \begin{itemize}
        \item Can handle NVMe SSD for instance and net storage, transparent encryption
        \item Nitro hypervisor not involved in tasks for networking and storage
        \item In OS Elastic Network Adapter driver
        \item Security groups implemented in the NIC
    \end{itemize}
    \item Can run bare metal
\end{itemize}

\section{Containerisation}
A \textbf{docker} is essentially a shipping container system for code. They eliminate the problem of running code on loads of different platforms by simply shipping this container to a computer, and then the application can be run locally.

Containers also bring:
\begin{itemize}
    \item Reproducability
    \item Portability
    \item Flexibility
    \item Isolation
\end{itemize}

It makes things like experiments really easy, because the runtime environment is always the same.

\textit{Container systems} use a number of components:
\begin{itemize}
    \item Linux containers (LXC)
    \begin{itemize}
        \item Cgroups and namespaces
    \end{itemize}
    \item Container runtimes
    \begin{itemize}
        \item Executables that read the container runtime specification, configure the kernel
    \end{itemize}
    \item Container images
    \begin{itemize}
        \item Applications
    \end{itemize}
    \item Container storage
    \begin{itemize}
        \item Linux storage systems used to store containter images on copyon-write (COW) file systems
    \end{itemize}
    \item Container registries
    \begin{itemize}
        \item Web servers used to store container images
    \end{itemize}
    \item Container engines
    \begin{itemize}
        \item Container tools used to pull images from container registries and assemble them on the host before creating the runtime specification and running the container runtime
    \end{itemize}
    \item Container image builders
    \begin{itemize}
        \item Tools used to create container images
    \end{itemize}
\end{itemize}

Docker is the most popular container system:
\begin{itemize}
    \item A container image format spec
    \item Tools for building container images
    \item Tools to manage container images
    \item Tools to manage instances of containers
    \item A system to share container images
    \item Tools to run containers
\end{itemize}

The \textbf{Open Container Initiative} (OCI) created an open governance structure creating industry standards around container formats and runtime. The runtime specification and the image specification were also created as part of this.

On the client computer, you interact with the docker via a CLI. An image is built (or pulled or runned) on the docker daemon. These images are a read only template with instructions for creating a Docker container. You can provide a Dockerfile that will determine how the image is run. The \textbf{container} itself is a runnable instance of this image. They are \textit{ephemeral}, meaning that unless specific action is taken, any changes that are not stored to a mounted persistent storage volume are lost.

\subsection{Namespaces}
Namespaces let you virtualise system resources like the file system or netowrking for each container. Each kind of namespace applies to a specific resource and each namespaec creates barriers between processes:
\begin{itemize}
    \item \textbf{pid namespace}: Responsible for isolating hte process (Process ID)
    \item \textbf{net namespace}: Manages network interfaces
    \item \textbf{ipc namespace}: Manages IPC resources (IPC: InterProcess Communiccation)
    \item \textbf{mnt namespace}: Manages the filesystem mount points (MNT: mount)
    \item \textbf{uts namespace}: Isolates kernel and version identifiers and hostname (UTS: Unix Timesharing System).
\end{itemize}

\subsection{Control groups}
Cgroups provide a way to limit access to resources such as CPU and memory that each container can use. Each time a new container is named, a cgroup of the same name appears.

\subsection{Container images}
They are standard TAR files with a base image and layered with differences. The base image contains:
\begin{itemize}
    \item Rootfs (container root filesystem)
    \item JSON file (container configuration)
\end{itemize}

\subsection{COW systems}
Instead of overwriting data, data is written somewhere else meaning better recovery. There is built in transactions also. There are snapshots, since we just take the current number of written layers and store a reference to that. If you make a modification to this, we can reconfigure any system with this reference point. Using this means that multiple containers can share the same base images, and just use the reference to the configuration they use.

\subsection{Performance}
Containers are almost always better than virtual machines. While they both emulate infrastructure and encapsulate the tenant, they differ in key ways:
\begin{itemize}
    \item VMs provide hardware level virtualisation, while containers provide OS level
    \item VMs need tens of seconds of provisioning, while containers only require milliseconds
    \item Virtualisation performance is slower than containers in most dimensions except networking
    \item VM tenants are fully isolated while containers provide process level isolation to tenants.
\end{itemize}

\section{Cloud Native Applications}
\textit{Cloud Native} is container based and elastic. AWS's biggest customer is \textbf{Netflix}. They roll out so many instances and microservices, and there is roughly 1 application change per second, meaning new features are coming out very frequently.

Microservices are the opposite of `monolithic applications' and support organisational agility, such as rolling releases and hot swapping, without any down time. This is achieved through horizontal auto-scaling, design for failure, modular design (containerised etc.). APIs are also used, as well as automating things.

A \textit{monolithic} application is built for one big block. Microservices aim to break this apart, where we have modules separated by API gateways.

The advantages of using this is that the services can be deployed to a subset of your users (called \textit{canary} deployment). This allows testing for small groups of people before rolling them out to everyone. These changes can be tiny; changing the size or colour of a button to see if it changes the click-through rate. Another deployment method is called \textit{blue/green}, where you have an example of your user base, and then you flip a switch and everyone has the new features instantly.

Distributed systems are harder to program, since remote calls are slow and always at risk of failure. There is also the issue of \textbf{eventual consistency}. Maintaining strong consistency is really difficult for a distributed system, meaning everyone has to manage this.

\subsection{Observability}
Once you have a massive cluster, how do you keep track of what's going on inside? Observability is a measure of how well internal states of a system can be inferred from knowledge of its external outputs.

\textbf{Logging} is a very useful method for observation. Local log files are useful at first, but as containers are ephemeral (deleted upon termination), if an instance crashes, you won't be able to retrieve the log data. Additionally, the log could grow potentially unlimited. \textit{Aggregated logs} are useful, and is what is used more commonly. It is a central server that records the log requests. Currently, a \textit{side car} approach is used for logstashes, meaning they have access to the same mounted volumes etc, and are an aid to the program.

\subsection{Metrics}
Metrics are more than logs. This checks the CPU, load average, interrupts, etc. and is able to identify bottlenecks. This is \textit{time sensitive}, so is stored in a time sensitive database. There is a plugin for Kubernetes that runs as a pod.

\section{Serverless Systems}
Firstly, it's important to know that serverless systems \textbf{still use servers!!} The essence of the serverless trend is the \textit{absence} of the server concept during software development. This allows developers to focus more time on things that will \textit{differentiate} their business from other businesses, rather than worry about the inner workings of a server. AWS defines serverless application as one that doesn't require you to provision or manage any servers. Unfortunately, this does lend itself to vendor lock-in.

Serverless systems have lots of possible usages, such as computation, data, messaging, user management and logging.

\subsection{The Four Pillars of Serverless}
\begin{enumerate}
\item No server management
\item Flexible scaling
\item No idle costs
\item High availability
\end{enumerate}

When you think of serverless, the most common flavour is AWS Lambda. The Lambda flavour operates with \textbf{two sets of permissions}, one for the invocation, and one for the service (or resource).

\subsection{Invocation}
A Lambda function is invoked via event or schedule. Minimal Amazon Linux data is run containing the required runtime and function. Function provided with JSON invocation context and parameters. The function executes or times out. Result returned if defined. It is typically invoked in a few ms (also called a warm start) but can take around 1000ms sometimes (cold start). The container is retained for a short time to mitigate this latency.

\subsection{Billing}
Billing for Lambda systems depends on the amount of memory allocated and duration (for invocation). Therefore, the language used for scripting has a big impact on this. For example, Java would cost a lot due to its heavyweight libraries, while Python or Ruby would cost much less, being very lightweight.

\subsection{The Four Stumbling Blocks of Serverless}
\begin{enumerate}
\item Performance Limitations (cold start etc)
\item Monitoring and Logging (closed source, so difficult to expand existing tools)
\item Vendor lock-in --- One of the worst forms of vendor lock in
\item Security and Privacy --- All data written is encrypted by Lambda, but in multi-tenant cloud systems, there is a lot of suspicion.
\end{enumerate}



\end{document}