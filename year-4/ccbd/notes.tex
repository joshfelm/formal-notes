% Preamble {{{
\documentclass[11pt,a4paper,titlepage,dvipsnames,cmyk]{scrartcl}
\usepackage[english]{babel}
\typearea{12}
% }}}

% Set indentation and line skip for paragraph {{{
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}
\usepackage[margin=2cm]{geometry}
\addtolength{\textheight}{-1in}
\setlength{\headsep}{.5in}
% }}}

\usepackage{hhline}
\usepackage[table]{xcolor}
\usepackage{mathtools}
\usepackage{tabularx}
\usepackage[T1]{fontenc}

% Headers setup {{{
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Cloud Computing and Big Data}
\rhead{Josh Felmeden}
\usepackage{hyperref}
% }}}

% Listings {{{
\usepackage[]{listings}
\lstset
{
    breaklines=true,
    tabsize=3,
    showstringspaces=false
}

\definecolor{lstgrey}{rgb}{0.05,0.05,0.05}
\usepackage{listings}
\makeatletter
\lstset{language=[Visual]Basic,
backgroundcolor=\color{lstgrey},
frame=single,
xleftmargin=0.7cm,
frame=tlbr, framesep=0.2cm, framerule=0pt,
basicstyle=\lst@ifdisplaystyle\color{white}\footnotesize\ttfamily\else\color{black}\footnotesize\ttfamily\fi,
captionpos=b,
tabsize=2,
keywordstyle=\color{Magenta}\bfseries,
identifierstyle=\color{Cyan},
stringstyle=\color{Yellow},
commentstyle=\color{Gray}\itshape
}
\makeatother
\renewcommand{\familydefault}{\sfdefault}
\newcommand{\specialcell}[2][c]{%
\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}
% }}}

% Other packages {{{
\usepackage{graphicx}
\graphicspath{ {./pics/} }
\usepackage{needspace}
\usepackage{tcolorbox}
\usepackage{soul}
\usepackage{babel,dejavu,helvet}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{tcolorbox}
\usepackage[symbol]{footmisc}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{enumitem}
\setlist{nolistsep}
% }}}

% tcolorbox {{{
\newtcolorbox{blue}[3][] {
colframe = #2!25,
colback = #2!10,
#1,
}

\newtcolorbox{titlebox}[3][] {
colframe = #2!25,
colback = #2!10,
coltitle = #2!20!black,
title = {#3},
fonttitle=\bfseries
#1,
}
% }}}

% Title {{{
\title{Cloud Computing and Big Data}
\author{Josh Felmeden}
% }}}

\begin{document}
\maketitle
\tableofcontents
\newpage
\section{Overview}
\subsection{Economic Driving Factors}
Cloud computing works by charging someone to use a service for a certain amount of time. If, for example, you let someone use your computer, you might charge them for their usage (if you were a real meanie). How that would be worked out is the \textit{operation expenditure} (Opex) and the \textit{capital expenditure} (Capex, cost of the computer). Combining these, we calculate the \textit{total cost of operation} and calculate the cost per day of the lifespan of the computer (in this example). So, if you used the computer for an hour, you would owe the person a 24th of this daily cost.

Now, this might come down to half a penny, but of course, this doesn't exist any more, so we might charge a whole penny instead. This seems like a marginal profit, but in terms of percentages, this is a $100\%$ increase. The fundamentals of this are how cloud computing generates so much income.

The attraction of this is that the users of the services do not have to pay the Capex, and simply pay opex for the rental of the service.

\subsection{Normal Failure}
Failures in these systems are to be expected. Say the servers you are buying are guaranteed to have a $99.999\%$ 3-year survival rate (`five-nines reliability'). This is good because there is a very high chance that this remains. Now, if you buy 10 of these servers, the probability that you have all of the servers still working is only $99.99\%$. Taking this to the extremes, if you buy 500,000 servers (this is standard practise for a lot of the big servers these days), the probability that all of them are still working is a measly $1\%$. Essentially, failure is something that should be normalised.

\textit{Modular data centres} are used a lot in big data centres. A unit may be left in the shipping container, and this container is removed or added as a container, meaning that if one module fails, another can just be replaced, meaning that the whole centre doesn't collapse.

In the current climate, the modular centres are considered a whole working unit, and this way of thinking was mobilised by a group of Google engineers.

\subsection{Blank as a service}
\begin{itemize}
    \item \textbf{SaaS: Software as a service}
    \begin{itemize}
        \item End user application software that is remotely delivered over the internet.
        \item Adobe is an example of this (Adobe Creative Cloud)
        \item Used to be bought off the shelf as a CD
    \end{itemize}
    \item \textbf{PaaS: Platform as a Service}
    \begin{itemize}
        \item Developer application software (middleware) functionality is remotely accessible
        \item Might provide a particular combination of OS, web-server, data-base and scripting
        \item Popular instance is the free `LAMP stack' (Linux, Apache, MYSQL and PHP)
        \item Used to be dominated by \textit{Google}
    \end{itemize}
    \item \textbf{IaaS: Infrastructure as a Service}
    \begin{itemize}
        \item IT infrastructure almost always virtualised and remotely accessible.
        \item Virtualisation software allows one physical server to be used by multiple users, each on a virtual machine. If one crashes, the others keep running.
        \item Used to be dominated by \textit{AWS} (although this is now a much bigger thing).
    \end{itemize}
\end{itemize}

While Google and Amazon dominated their respective fields, both of these companies have expanded into the other fields. This is very complicated, but the key thing is that both Google and Amazon offer both PaaS and IaaS.

In around 2015, Amazon created \textbf{FaaS: function as a service (aka `serverless')}:
\begin{itemize}
    \item No server processes visibly running. Pay only for the time spent executing a function
    \item UNlike PaaS, scale out without increasing number of servers.
    \item Amazon Lambda is the best known example, although both Google and Microsoft have answers to this.
\end{itemize}

\subsection{Impacts of Cloud Services}
Cloud services have revolutionised business in many ways. It is now possible to do tasks that would previously require access to high performance machines. Instead, it can be sent to a big data centre, and this usage is just charged as rental.

\textit{Interoperability} is a big issue. \textbf{Vendor lock-in} is a concern for a lot of clients, and this simply means that once a client is locked into a vendor, it becomes financially or practically unviable to switch to another supplier. This led to an attempt to develop a sense of unity between cloud companies, where companies created the \textit{Open Cloud Manifesto}. A lot of big companies signed up to this, but a lot of the top dogs didn't sign up for this (unsurprisingly, Google and Amazon). This manifesto seemingly no longer exists as an original. As a consumer, this is bad and worrying because vendor lock in is very possible.

\section{IaaS and AWS}
\subsection{Amazon Simple Storage Service}
Amazon S3 is a cloud-based persistent storage. It operates independently from other Amazon services. The simple refers to the features, not that it's simple to use. You can store data in the cloud. You also don't store files, you store \textit{objects}, and these are kept in buckets. Objects have a size limit (5Tb) and a max size on a single upload is 5Gb. All buckets share the same namespace, so no sub-buckets.

It's very easy to use; just use a web GUI that is similar to AWS. It also has a command line interface and has scripting. Default storage can be selected (geographically).

S3 is accessed via API, either by SOAP (xml) or REST (http). Wrappers are available to abstract the API for programmers.

This is just the storage, so now we will look at the computing of data in the cloud.

\subsection{Amazon Elastic Compute Cloud (EC2)}
This is a remotely accessible virtual network of virtual servers. Usually, EC2 is run with S3 providing the storage.

A single EC2 virtual server, with the chosen OS etc, is an instance. An instance is instantiatied from an Amazon Machine Image (AMI)
\begin{itemize}
    \item One AMI can be cloned $n$ times to create $n$ instances
    \item You can build your own by cloning an AMI from your local server
    \item Or, Amazon have a bunch of prebuilt AMIs that you can choose from
\end{itemize}

EC2 dynamically assigns a unique IP address to each instance, and this IP can be reassigned, perhaps to someone else. The IP can also be static (also known as an Elastic IP address) at a cost.

EC2 instances run in availability zones (AZs), grouped into regions. AZ is similar to a single data centre, guaranteeing an area has 99.95\% uptime.

\subsubsection{Usage}
Some basic API routes as S3, command-line or a bit of GUI too:
\begin{itemize}
    \item Amazon's own AWS web console
    \item Various EC2 plug-ins for browsers
    \item Third-party cloud management tools
\end{itemize}

Some AMIs are junk or malware, however, so be careful when selecting this.

There are three types of storage:
\begin{itemize}
    \item Ephemeral local storage in the instance (dies with instance)
    \item Persistent cloud (S3)
    \item SAN-style Elastic Block Storage (EBS)
    \begin{itemize}
        \item Allows user to create volumes from 1Gb to 1Tb
        \item Any number of volumes may be mounted from a single instance
    \end{itemize}
\end{itemize}

S3 is slow, medium-reliable, but super-durable. Never loses data, so is good for DR backups. Instance storage is simple and cheap, but speed can be really poor. EBS is high on everything, but is complex and costly.

There is some autoscaling based on matrics:
\begin{itemize}
   \item \textbf{Cloudwatch}: automated monitoring of EC2 instances. Reveals many statistics such as CPU utilisation, disk reads/writes and network traffic. Aggregates and stores monitoring data that can be accessed
   \item \textbf{Auto Scaling}: dynamically adds or removes EC2 instances based on CloudWatch metrics. You define conditions upon which you want to scale up or down your EC2 instances. Auto scaling automatically adds or removes the specified amount of Amazon EC2 instances when it detects that the conditions have been met.
   \item \textbf{Elastic Load Balancing}: automatically distributes incoming application traffic across multiple EC2 instances. Better fault tolerance. Elastic Load Balancing detects unhealthy instances within a pool and automatically reroutes traffic to healthy instances until the unhealthy instances have been restored. Customers can enable ELB within a single AZ or across multiple for consistent application performance.
\end{itemize}

\subsection{AWS Simple Queue Service SQS and Architecting for Scale-out}
SQS is reliable, loosely-coupled fault-tolerant storage and delivery of messages. It can be between any clients or computers connected to the internet, and senders and recipients do not have to communicate directly. No requirement that either side be always-available or connected to the internet.

A \textbf{message} is up to 256Kb of text-data, sent to SQS and stored until it is delivered. A \textbf{queue} serves to group related messages together.

SQS is accessible to clients on any HTTP-enabled platform. Messages are stored redundantly over multiple data-centres truly distributed. Unfortunately, this brings about a few down-sides:
\begin{itemize}
    \item Message retrievals may be incomplete
    \item Messages may not be delivered quickly (2-10 seconds)
    \item Messages may be delivered out of order
    \item Messages may be redelivered
\end{itemize}

\subsubsection{Mitch Garnaat's Monster Muck Mashup}
This is a service that converts AVI videos to mp4 using:
\begin{itemize}
    \item `Boto' Python interface to AWS
    \item S3 to store the video files
    \item EC2 to do the conversion processing
    \item SQS for inter-process communication
\end{itemize}

Uses AWS for \textit{scalability} (scale-out not up, not just buying new resources, adding cheap machines to improve ability.)

The basic steps are:
\begin{enumerate}
\item Upload a bunch of video files to a S3 bucket
\item For each file, add a msg to SQS input queue
\item On the EC2 instance, repeat this until input queue is empty:
\begin{enumerate}
    \item Read message MI from input queue
    \item Retrieve from S3 the video VI specified in MI
    \item Do the conversion creating VO
    \item Store VO in S3
    \item Write message MO to SQS output queue
    \item Delete MI from input queue
\end{enumerate}
\end{enumerate}

This really illustrates how good AWS is at scaling software. It is good because any number of clients can connect to the bucket. If this bucket is 100\% full, it doesn't matter, because additional instances can all talk to the same buckets and queues, so the workload is met.

\subsection{AWS simpleDB and AWS Relational Databases (RDB)}
\subsubsection{Amazon SimpleDB}
This software provides:
\begin{itemize}
    \item Reliable storage of structured textual data
    \item Languages that allows you to store, modify, query, and retrieve data sets
    \item Automatic indexing of all stored data
\end{itemize}

SimpleDB provides 3 main resources:
\begin{itemize}
    \item \textbf{Domains}: Highest-level container for related data \textit{items}: queries only search within one domain
    \item \textbf{Items}: a named collection of \textit{attributes} that represent a data object. Each item has a unique name within the domain; items can be created, modified, or deleted; individual attributes within an an item can be manipulated
    \item \textbf{Attributes}: an individual category of information within the item, with a name unique for that item. Item has one or more text string values associated with the name
\end{itemize}

The downside to this type of storage is that it really does only do one data type (textual). If, for example, you wanted to store pi, you would need to store it as a character string (`3', `.', `1').

There is no Database \textbf{schema}, meaning if you mistype something, the database will just accept it as a definition, leading to some unfortunate results. It is not a traditional relational database management system:
\begin{itemize}
    \item SimpleDB items are stored in a hierarchical structure, not a table
    \item SimpleDB attribute value max size is 1Kb
    \item SimpleDB data is all stored as text
    \item The query language is really basic
    \item SimpleDB is distributed, so data consistency may suffer due to propagation delays
\end{itemize}

\subsubsection{Amazon Relation Database Service}
There is also a relational database system, possibly as a response to Azure. You can set up, operate and scale a full MySQL RDBMS without having to worry about infrastructure provisioning, software maintenance, or common DB management tasks, like backups.

The processing power and storage space can be scaled as needed with a single API call and you can fully initiate fully consistent database snapshots at any time. Can import a dump file to get started. Each DB instance exports a number of metrics to CloudWatch including CPU utilisation, etc.

\subsection{Availability Zones}
Availability zones are clusters of independent data centres that are up to 20 miles apart. They are interconnected using low latency and enable fault isolation and HA.

Choosing which region to use comes down to a few reasons:
\begin{itemize}
    \item Data sovereignty and compliance
    \begin{itemize}
        \item Where are you storing user data?
    \end{itemize}
    \item Proximity of users to data
    \item Services and feature availability
    \item Cost effectiveness
    \begin{itemize}
        \item Each region has differing costs
    \end{itemize}
\end{itemize}

High Availability (HA) is the ability to minimise service downtime by using redundant components. It also requires service components in at least two AZs.

Fault tolerance is the same as HA, but also the ability to ensure that no service disruption by using active-active architecture meaning that all components are active all the time. This is of course a lot more costly that just having HA.

IaaS may have HA, but FT unlikely. PaaS will usually have HA, but some services offer FT.

\section{Virtualisation}
The most fundamental type of cloud computing is IaaS compute, and the most fundamental type of this is the virtual machine. It has unmanaged services, which means you control what they do. You create, save or reuse them. They are networked and connected to storage and have certain security systems.

EC2 instances are a form of virtualisation.
\begin{itemize}
    \item They mostly run on Xeon processors, but also have other processors available.
    \item There are lots of different tiers available on AWS that use different purposes
    \item They run in AMIs.
    \item After creating a virtual machine, you can start, stop, and terminate the machines. Once the machine is terminated, it will not come back.
\end{itemize}

In virtualisation, there is some virtual memory that points to addresses in physical memory. It is an abstraction of the storage resources, because the virtual memory seems contiguous, but in physical memory it could be in various locations. The operating system manages this and also has hardware support.

\begin{center}
    \begin{tabularx}{\textwidth}{X|X}
        \textbf{Virtual Memory} & \textbf{Virtual Machine} \\ \midrule
        Abstraction of the RAM memory resources & Abstraction of the storage/process/IO resources. \\
        Mapping of program (virtual) memory addresses to physical addresses & Time slicing VM use of virtual memory addresses/CPU/IO registers in physical addresses. \\
        Operating system manages & Operating system/Hypervisor manages \\
        Hardware support (memory management unit) & Hardware support (e.g. Intel VT)
    \end{tabularx}
\end{center}

\subsection{The hypervisor}


This virtualisation is not a new idea. It has been in use since around 1960, but it has been formalised in 1974, where they defined three important properties:
\begin{itemize}
    \item \textit{Fidelity}: Program gets the same output whether on VM or hardware
    \item \textit{Performance}: Performs close to physical computer
    \item \textit{Safety}: Cannot change or corrupt data on physical computer
\end{itemize}

This was ensured by analysing the instruction sets and identified two instructions that are the most important: \textbf{sensitive} instructions (that can change configurations of resources) and \textbf{privileged} instructions. Sensitive instructions need to be caught by the operating system, jumping from user mode to kernel mode. Therefore, all sensitive instructions need to be a subset of privileged instructions. This is all handled by the hypervisor.

Therefore, we have the hypervisor in contact with the server hardware, and then on top of this, we have the VM (or multiple machines) that consist of a guest OS, middleware, and apps. This type of VM is called \textbf{bare metal}.

Another type, called \textbf{hosted}, has a host OS running on top of the server hardware, and then the hypervisor.

The difference between the two is clear; in type two, the hypervisor can write to the host OS meaning that it can be more lightweight, where as type one can be much faster, at the cost of having to contain stuff that is normally handled by the OS.

\subsubsection{Simulated Hardware}
\textit{Full virtualisation} is a complete or almost complete simulation of the underlying guest-machine hardware: virtualised guest OS runs as if it were on a bare machine.

The alternative to this is \textit{paravirtualisation} and is only possible when the source code of the OS is available. The guest OS is edited and recompiled to make system calls into the hypervisor API to execute safe rewrites of sensitive instructions. In this example, the hypervisor doesn't simulate hardware.

EC2 offers both of these virtual machines:
\begin{itemize}
    \item HVM --- hardware virtual machine
    \begin{itemize}
        \item Virtualised set of hardware
        \item Can use OS without modification
        \item Intel virtualisation technology
    \end{itemize}
    \item PV --- Paravirtualisation
    \begin{itemize}
        \item Requires OS to be prepared
        \item Doesn't support GPU instances
    \end{itemize}
\end{itemize}

\subsubsection{Xen}
Xen is a free, open-source hypervisor developed at University of Cambridge. It has multiple modes:
\begin{itemize}
    \item Paravirtualisation: guest OS recompiled with modifications
    \item Hardware assisted virtualisation: Intel x86 and ARM extensions
    \item Widespread
    \item Not easily virtualisable (17 instructions that violate the sensitive rules above)
\end{itemize}

\subsubsection{Areas of Virtualisation}
There are three main areas of responsibility for virtualisation managers:
\begin{itemize}
    \item CPU virtualisation
    \begin{itemize}
        \item Guest has exclusive use of a CPU for a period of time
        \item CPU state of the first guest is saved, and the state of the next guest is loaded before control is passed to it
    \end{itemize}
    \item Memory virtualisation
    \begin{itemize}
        \item Additional layer of indirection to virtual memory
    \end{itemize}
    \item I/O virtualisation
    \begin{itemize}
        \item Hypervisor implements a device model to provide abstractions of the hardware
    \end{itemize}
\end{itemize}

\subsubsection{KVM}
Converts Linux into a type-1 hypervisor. Memory manager, process scheduler, I/O stack from Linux. The VM is implemented as a regular Linux process. KVM requires CPU virtualisation extensions to handle instructions.

\subsubsection{Nitro Hypervisor}
\begin{itemize}
    \item Based on KVM
    \item Offloads virtualisation functions to dedicated hardware and software
    \item Hypervisor mainly provides CPU and memory isolation to EC2 instances
    \item Nitro cards for VPC networking and EBS storage
    \begin{itemize}
        \item Can handle NVMe SSD for instance and net storage, transparent encryption
        \item Nitro hypervisor not involved in tasks for networking and storage
        \item In OS Elastic Network Adapter driver
        \item Security groups implemented in the NIC
    \end{itemize}
    \item Can run bare metal
\end{itemize}

\section{Containerisation}
A \textbf{docker} is essentially a shipping container system for code. They eliminate the problem of running code on loads of different platforms by simply shipping this container to a computer, and then the application can be run locally.

Containers also bring:
\begin{itemize}
    \item Reproducability
    \item Portability
    \item Flexibility
    \item Isolation
\end{itemize}

It makes things like experiments really easy, because the runtime environment is always the same.

\textit{Container systems} use a number of components:
\begin{itemize}
    \item Linux containers (LXC)
    \begin{itemize}
        \item Cgroups and namespaces
    \end{itemize}
    \item Container runtimes
    \begin{itemize}
        \item Executables that read the container runtime specification, configure the kernel
    \end{itemize}
    \item Container images
    \begin{itemize}
        \item Applications
    \end{itemize}
    \item Container storage
    \begin{itemize}
        \item Linux storage systems used to store containter images on copyon-write (COW) file systems
    \end{itemize}
    \item Container registries
    \begin{itemize}
        \item Web servers used to store container images
    \end{itemize}
    \item Container engines
    \begin{itemize}
        \item Container tools used to pull images from container registries and assemble them on the host before creating the runtime specification and running the container runtime
    \end{itemize}
    \item Container image builders
    \begin{itemize}
        \item Tools used to create container images
    \end{itemize}
\end{itemize}

Docker is the most popular container system:
\begin{itemize}
    \item A container image format spec
    \item Tools for building container images
    \item Tools to manage container images
    \item Tools to manage instances of containers
    \item A system to share container images
    \item Tools to run containers
\end{itemize}

The \textbf{Open Container Initiative} (OCI) created an open governance structure creating industry standards around container formats and runtime. The runtime specification and the image specification were also created as part of this.

On the client computer, you interact with the docker via a CLI. An image is built (or pulled or runned) on the docker daemon. These images are a read only template with instructions for creating a Docker container. You can provide a Dockerfile that will determine how the image is run. The \textbf{container} itself is a runnable instance of this image. They are \textit{ephemeral}, meaning that unless specific action is taken, any changes that are not stored to a mounted persistent storage volume are lost.

\subsection{Namespaces}
Namespaces let you virtualise system resources like the file system or netowrking for each container. Each kind of namespace applies to a specific resource and each namespaec creates barriers between processes:
\begin{itemize}
    \item \textbf{pid namespace}: Responsible for isolating hte process (Process ID)
    \item \textbf{net namespace}: Manages network interfaces
    \item \textbf{ipc namespace}: Manages IPC resources (IPC: InterProcess Communiccation)
    \item \textbf{mnt namespace}: Manages the filesystem mount points (MNT: mount)
    \item \textbf{uts namespace}: Isolates kernel and version identifiers and hostname (UTS: Unix Timesharing System).
\end{itemize}

\subsection{Control groups}
Cgroups provide a way to limit access to resources such as CPU and memory that each container can use. Each time a new container is named, a cgroup of the same name appears.

\subsection{Container images}
They are standard TAR files with a base image and layered with differences. The base image contains:
\begin{itemize}
    \item Rootfs (container root filesystem)
    \item JSON file (container configuration)
\end{itemize}

\subsection{COW systems}
Instead of overwriting data, data is written somewhere else meaning better recovery. There is built in transactions also. There are snapshots, since we just take the current number of written layers and store a reference to that. If you make a modification to this, we can reconfigure any system with this reference point. Using this means that multiple containers can share the same base images, and just use the reference to the configuration they use.

\subsection{Performance}
Containers are almost always better than virtual machines. While they both emulate infrastructure and encapsulate the tenant, they differ in key ways:
\begin{itemize}
    \item VMs provide hardware level virtualisation, while containers provide OS level
    \item VMs need tens of seconds of provisioning, while containers only require milliseconds
    \item Virtualisation performance is slower than containers in most dimensions except networking
    \item VM tenants are fully isolated while containers provide process level isolation to tenants.
\end{itemize}

\section{Application Orchestration and Kubernetes}
Orchestration is a number of things:
\begin{itemize}
    \item Running containers at scale requires management tools
    \item Manage networking volumes, infrastructure
    \item Automate
    \begin{itemize}
        \item Fault tolerance, self-healing
        \item Auto scaling on demand
        \item DevOps
        \item Update/rollback without downtime
    \end{itemize}
    \item The tools available for us to do this are Mesos, Docker swarm, and Kubernetes
\end{itemize}

\subsection{Kubernetes}
The features of Kubernetes include:
\begin{itemize}
    \item Automatic scheduling of work based on resource usage and constraints
    \item Self-healing: automatic replacement and rescheduling of failed containers
    \item Service discovery and load balancing
    \item Automated rollouts and rollbacks
\end{itemize}

The most important things to get to grips with are the runtime objects: pods, deployments, and services.

\subsubsection{Pod}
A pod is a set of one or more containers that act as a unit and are scheduled onto a node together. They share a local network and can share file-system volumes.

\subsubsection{Deployments}
These describe the \textit{desired state} through declarative updates for pods and ReplicaSets. Essentially, you describe what you want, submit this to Kubernetes, and then it is deployed how you want (without actually telling it exactly what you want). The ReplicaSets balance the number of scheduled and running pods (kills or creates).

The deployment is managed via the spec (what we want), the monitors status (current state), and the template (how).

You are able to create a deployment to rollout a ReplicaSet, declare the new state of the pods, rollback to a previous version, or scale up the deployment to cope with more load.

Deployments use \textbf{Yaml files} to describe them (similar to Python, in that it uses indentation to infer scope).

\subsubsection{Services}
Services define networking to access pods consistently. They also expose pods to the external world. They create groupings of pods that can be referred to by name and have a unique IP address and DNS hostname (these by default cluster scope only). The services ensure the pods that are in use are load balanced, and environment variables containing the IP address of each service in the cluster are injected into all the containers.

Services are defined by a \textit{service config file} (also in Yaml).

There are a few service types:
\begin{itemize}
    \item Influences networking configuration
    \item Cluster IP
    \begin{itemize}
        \item Default
        \item Service is discoverable/routable only within the cluster
        \item kube-proxy watches API service and updates pod IPTables on change
    \end{itemize}
    \item NodePort
    \begin{itemize}
        \item Exposes the service on each Node's IP at a static port (the NodePort)
        \item Access the service via <NodeIP>:<NodePort>
        \item The simplest way to make your service externally accessible
    \end{itemize}
\end{itemize}
\section{Cloud Native Applications}
\textit{Cloud Native} is container based and elastic. AWS's biggest customer is \textbf{Netflix}. They roll out so many instances and microservices, and there is roughly 1 application change per second, meaning new features are coming out very frequently.

Microservices are the opposite of `monolithic applications' and support organisational agility, such as rolling releases and hot swapping, without any down time. This is achieved through horizontal auto-scaling, design for failure, modular design (containerised etc.). APIs are also used, as well as automating things.

A \textit{monolithic} application is built for one big block. Microservices aim to break this apart, where we have modules separated by API gateways.

The advantages of using this is that the services can be deployed to a subset of your users (called \textit{canary} deployment). This allows testing for small groups of people before rolling them out to everyone. These changes can be tiny; changing the size or colour of a button to see if it changes the click-through rate. Another deployment method is called \textit{blue/green}, where you have an example of your user base, and then you flip a switch and everyone has the new features instantly.

Distributed systems are harder to program, since remote calls are slow and always at risk of failure. There is also the issue of \textbf{eventual consistency}. Maintaining strong consistency is really difficult for a distributed system, meaning everyone has to manage this.

\subsection{Observability}
Once you have a massive cluster, how do you keep track of what's going on inside? Observability is a measure of how well internal states of a system can be inferred from knowledge of its external outputs.

\textbf{Logging} is a very useful method for observation. Local log files are useful at first, but as containers are ephemeral (deleted upon termination), if an instance crashes, you won't be able to retrieve the log data. Additionally, the log could grow potentially unlimited. \textit{Aggregated logs} are useful, and is what is used more commonly. It is a central server that records the log requests. Currently, a \textit{side car} approach is used for logstashes, meaning they have access to the same mounted volumes etc, and are an aid to the program.

\subsection{Metrics}
Metrics are more than logs. This checks the CPU, load average, interrupts, etc. and is able to identify bottlenecks. This is \textit{time sensitive}, so is stored in a time sensitive database. There is a plugin for Kubernetes that runs as a pod.

\section{Serverless Systems}
Firstly, it's important to know that serverless systems \textbf{still use servers!!} The essence of the serverless trend is the \textit{absence} of the server concept during software development. This allows developers to focus more time on things that will \textit{differentiate} their business from other businesses, rather than worry about the inner workings of a server. AWS defines serverless application as one that doesn't require you to provision or manage any servers. Unfortunately, this does lend itself to vendor lock-in.

Serverless systems have lots of possible usages, such as computation, data, messaging, user management and logging.

\subsection{The Four Pillars of Serverless}
\begin{enumerate}
\item No server management
\item Flexible scaling
\item No idle costs
\item High availability
\end{enumerate}

When you think of serverless, the most common flavour is AWS Lambda. The Lambda flavour operates with \textbf{two sets of permissions}, one for the invocation, and one for the service (or resource).

\subsection{Invocation}
A Lambda function is invoked via event or schedule. Minimal Amazon Linux data is run containing the required runtime and function. Function provided with JSON invocation context and parameters. The function executes or times out. Result returned if defined. It is typically invoked in a few ms (also called a warm start) but can take around 1000ms sometimes (cold start). The container is retained for a short time to mitigate this latency.

\subsection{Billing}
Billing for Lambda systems depends on the amount of memory allocated and duration (for invocation). Therefore, the language used for scripting has a big impact on this. For example, Java would cost a lot due to its heavyweight libraries, while Python or Ruby would cost much less, being very lightweight.

\subsection{The Four Stumbling Blocks of Serverless}
\begin{enumerate}
\item Performance Limitations (cold start etc)
\item Monitoring and Logging (closed source, so difficult to expand existing tools)
\item Vendor lock-in --- One of the worst forms of vendor lock in
\item Security and Privacy --- All data written is encrypted by Lambda, but in multi-tenant cloud systems, there is a lot of suspicion.
\end{enumerate}

\section{Scalable Cloud Architectures}
Software architecture is a set of structures needed to reason about a system. It includes software components, the relations between them and properties of both. The architecture may be implicit, meaning that it might not be written down, but every software has an architecture. If you build a significant system, you cannot not have an architecture.

Looking at scaling, we can either scale with pure cores, or use multiple servers (called vertical or horizontal scaling respectively). With \textbf{vertical scaling}, there will be no need to change your architecture, while \textbf{horizontal scaling} will be cheaper.

\subsection{Scaling cube}
The scale cube only looks at horizontal scaling. There are three dimensions to this cube:
\begin{itemize}
    \item X axis: Horizontal duplication --- Scale by cloning services and data
    \begin{itemize}
        \item Each clone can do the work of all the other clones. Work is distributed among clones without bias
        \item Inefficient compared to alternatives
        \item But, very easy to do
    \end{itemize}
    \item Y axis: Functional decomposition --- Scale by splitting different things. Isolating and making scalable individual responsibility of components
    \begin{itemize}
        \item Needs to be split in the code base
        \item More costly than the x axis.
    \end{itemize}
    \item Z axis: Data partitioning --- Scale by splitting similar things. We partition the domains of incoming requests
    \begin{itemize}
        \item Data partitioning split relative to the client
        \item Improves fault tolerance and cache  performance
        \item More costly than the other two
    \end{itemize}
\end{itemize}

\subsection{Load balancers}
Load balancers are used for:
\begin{itemize}
    \item Distributing requests
    \item Managing availability
    \item Performing health checks
    \item Session affinity (sticky sessions)
    \item List of backend servers
    \item Load balancing policy
    \item SSL termination
    \item Alternative: DNS or router based
\end{itemize}

Load balancers can detect denial of service attacks and shut servers down rather than scaling the service up to keep up with the increased traffic (that could end up costing a hell of a lot of money).

\textbf{Sticky sessions} are really useful. If we use a sticky load balancer, we can always route a client's request to the same server instance, perhaps with an IP hash. This sticky session is really good because say you order things on amazon and have a shopping cart full, we want the next request to go to the server that has just been handling this shopping cart. Cookies are managed by load balancers or the application cookie. It enables session replication via shared session DBs or caches. Sometimes, the session is stored with the client, though this can be risky as the internals of the application can become exposed to the client.

There are a few load balancing algorithms, such as:
\begin{itemize}
    \item Round robin --- Simple even task distribution (ignores the difference in work)
    \item Weighted round robin
    \item Least connections
\end{itemize}

Load balancers allow high availability. If one load balancer has a high load, it can switch to a secondary load balancer. Kubernetes provides stateless LB via a service (round-robin by default). Additionally, they have sticky sessions.

\subsection{Decoupling services}

\subsubsection{Message topics}
Consumers can subscribe to different message topics, meaning that producers just send a message to a pub/sub (publish/subscribe), and then only the consumers that are subscribed to a topic would receive this message. This generates a disconnect between the producers and the subscribers but also means more scalability because the producers don't need to worry about each individual consumer.

Message queues are also used, and they are \textit{asynchronous}, meaning it can be queued now and then run later, waiting until a consumer is ready to process. This decouples application logic and is also much more scalable, although it introduces latency into the system.

\subsubsection{Service registries}
Service registries resolve addresses for names and are based on HA, transactional data stores. For example, Apache.

These decoupling services allow for \textit{event driven} algorithms.

\subsection{Automation}
You can't manually scale instances in your architecture, so you need to automate this. You'll need an elasticity controller that looks at metrics (CPU, mem, disk...). When exceeded, add more nodes, and when it isn't using much, can decommission some of these nodes.

Another way of judging whether or not we need to scale a system is to look at a job queue and scale depending on this.

\subsection{Sharding}
Sharding is the act of partitioning and storing a dataset in multiple databases. This might occur when a database gets really full, because replicating this data wouldn't solve the problem, or it might also hold some redundant data. This is particularly useful when having multiple caches. If we have multiple caches and each cache has the same data, we will have a lot of cache \textit{misses}, so we are not getting the most bang for our buck.

\subsection{Decentralised Hashing}
Regular caches distribute evenly across buckets as a function of $n$. However, if $n$ changes, all objects need redistribution. The solution to this is \textit{consistent hashing}:
\begin{itemize}
    \item Each peer received similar amounts of keys
    \item Little reshuffling on peers entering or leaving
    \item Map each object to a point on the edge of a circle
    \item Make a node hold keys for a range of consecutive keys
    \item If a node is removed, its interval is taken over by a neighbour node. All the remaining caches are unchanged.
    \item If a new node enters, keys are redistributed from a preceding node. All remaining nodes are unchanged.
\end{itemize}

\subsection{Database scaling}
Database scaling is one of the harder things to scale. One technique is \textbf{read-replicas}, but this is not good for writing. Another example is \textbf{sharding} (looked at above). 

\section{Coordination in Distributed Systems}
Errors in distributed systems can either be a \textbf{fault}, which is a single component misbehaving, and a \textbf{failure}, which is where the whole system stops. If we have a data center with 10000 hard drives, the probability that one hard drive fails per day is almost 1. Additionally, communication and clocks are unreliable. This leads to a lot of potential errors. We can also get process pauses, such as garbage collection.

\subsection{Replication and Linearisability}
Once a value has been set for a key, all subsequent read operations return that value until a new value is set. This is a system that conforms to \textit{linearisability}.

\subsection{CAP theorem}
A good cloud might seek to achieve these three things:
\begin{itemize}
    \item Consistency
    \begin{itemize}
        \item All nodes should see the same data at the same time
    \end{itemize}
    \item Availability
    \begin{itemize}
        \item Node failures do not prevent survivors from continuing to operate
    \end{itemize}
    \item Partition tolerance
    \begin{itemize}
        \item The system continues to operate despite network partitions
    \end{itemize}
\end{itemize}

However, a bloke called Eric Bewer has a theorem that states that a cloud service cannot simultaneously provide these, and there will always be some kind of trade off.

Consider the following:
\begin{itemize}
    \item Nodes X and Y suffer a partition
    \item X wants to move forward with a job but gets no reply from Y
    \item X must now either
    \begin{itemize}
        \item wait to hear back from Y (sacrificing availability)
        \item Proceed without hearing back (threatening consistency)
        \item or never be partitioned in the first place (losing partition tolerance)
    \end{itemize}
\end{itemize}

The CAP theorem was understood to mean that the designer's job is to pick 2 out of 3. But, with cloud, we need partition-tolerance, so you have to choose between C and A. However, these three are not binary concepts. Each is a matter of degree, so we only rule out the possibility of `perfect' consistency and availability.

Let's now observe some trade-offs:
\begin{itemize}
    \item \textbf{Strong consistency} --- After an update completes, any subsequent access will return the same updated value.
    \item \textbf{Weak consistency} --- Subsequent accesses are not guaranteed to return the updated value
    \item \textbf{Eventual consistency} --- Eventually, all subsequent accesses will return the updated value (e.g. updates will eventually propagate to replicas).
\end{itemize}

This eventual consistency is good for something like a blog post. But in a different context, it might not be enough.

Prioritizing C could be used where a cloud supports operations in the real world that are really hard to roll back.

We might have a lot of replicas. Now, there are some problems we need to overcome when dealing with a lot of replicas, and these might be something like where multiple requests come in and it is unsure which one should be processed first. The way around this is to assign each `problem' a value, and then to choose a valid value at random. Not one that is `best' or came first, just a valid value. These replicas will have three main roles:

\begin{itemize}
    \item Proposers: learn values that are already accepted and propose these values
    \item Acceptors: let proposers know already accepted values, accept or reject proposals, and reach a consensus on choosing a particular proposal or value
    \item Learners: become aware of the chosen proposal or value and action it
\end{itemize}

Replicas may play more than one of these roles at different times. Often, a replica is elected to be a `distinguished' (or privileged) learner/proposer. This is where they are the only one allowed to play the role. Also, different proposals must never use the same ID number (obviously).

Here is how the algorithm unfolds:
\begin{itemize}
    \item Proposer P picks an ID number N, which is higher than any chosen before.
    \item P asks some majority of acceptors to prepare for proposal N.
    \item Only a majority and not all are asked because if a majority of replicas agree on a value, then a different majority cannot agree on a different one.
    \item At this stage, P hasn't proposed a value, and is only trying to engage enough acceptors
    \item Acceptors may or may not sign up for this
    \item If an acceptor A gets a prepare message from P with ID, N, bigger than any seen so far, then A promises to ignore future proposals with ID less than N.
    \item If A has already accepted a proposal, A tells P the value of the last accepted proposal.
    \item If N is too low, A ignore P.
    \item If P gets the majority of promises back, P can set a value.
    \item If the acceptors have told their accepted values, P chooses the value, v, associated with the highest proposal ID number. If no accepts so far, P sets value.
    \item P request acceptors accept this proposal
    \item They must accept unless P was too slow and already proposed another proposer.
    \item An acceptor will accept the first proposal it sees. System will not hang waiting to compare multiple ideas.
    \item An acceptor will accept the proposal with the highest ID number. This keeps the system going when proposers die mid proposal.
    \item A majority of acceptors must accept the same value keeping the system consistent
    \item Once a value is chosen, all proposals with a higher ID number choose to recommend this same value, meaning we can build a consensus without newcomers disrupting it.
\end{itemize}

\subsection{Apache Zookeeper}
Zookeeper is part of a family of linearisable, durable data stores, with a rich client that prioritises consistency over availability. There are typically five nodes in a cluster and are optimised for read, rather than write.

\section{DevOps}
Software development lifecycle:
\begin{enumerate}
\item Planning
\item requirements Definition
\item Software design
\item Software development
\item Software testing
\item Software deployment
\end{enumerate}

Before we had Agile and DevOps, we used to use the waterfall system. The main problem with this was that because it is very one way, it becomes very hard to change things later on in the development. As a result, the final product may not reflect customer requirements and becomes very expensive to change features and spec late in the process.

\textbf{Agile} became the favoured development method in the 1990s. It has iterative development cycles using `sprints'. Each of these sprints delivers a minimum viable product, each of the sprints usually being 2 weeks long. The product owner is the customer specifying feature requirements for each sprint and signing off releases. Agile methodology allows for quick iteration on feedback.

\textbf{DevOps} breaks the wall down between development and operations. The goal is simple: releasing updates as quickly as possible without losing quality. It's a solution to cultural norms: developers want \textit{agility} while operations want \textit{stability}. DevOps is an extension of the Agile methodology:
\begin{itemize}
    \item People: Break down Dev and IT Ops silos through improved communications and joint accountability for product quality
    \item Processes: Increase throughput and quality by automating manual processes
    \item Tools: Create an end-to-end scripted tool chain (pipeline) for speed and consistency.
\end{itemize}

DevOps uses cross functional Dev/Ops teams. This allows for:
\begin{itemize}
    \item Collaboration
    \begin{itemize}
        \item Sprint dev teams may include QA and IT Ops members
    \end{itemize}
    \item Responsibility
    \begin{itemize}
        \item Teams are responsible to end-to-end quality of product
    \end{itemize}
    \item Awareness
    \begin{itemize}
        \item Devs learn about potential Ops issues, Ops learn about code
    \end{itemize}
    \item Communication
    \begin{itemize}
        \item Teams use integrated comms tools like slack and JIRA to improve issue reporting and response
    \end{itemize}
\end{itemize}

Another benefit of DevOps is that there are frequent \textit{commits} and \textit{builds}. These builds should be automated so that they can be performed multiple times per day. These processes should also be automated:
\begin{itemize}
    \item Continuous Integration:
    \begin{itemize}
        \item Shared source code repository
        \item Automated unit testing on commit
        \item Frequent integration builds and tests
        \item \textbf{Ensures robust code base}
    \end{itemize}
    \item Continuous delivery
    \begin{itemize}
        \item CI + more testing + packaging
        \item \textbf{Ensures production-ready release}
    \end{itemize}
    \item Optionally, could go further with Continuous deployment
    \begin{itemize}
        \item CI + CD + automated deployment to prod environment
    \end{itemize}
\end{itemize}

The third benefit of DevOps is \textbf{robust product releases}:
\begin{itemize}
    \item Fail-Fast philosophy
    \begin{itemize}
        \item CI/CD automated tests pinpoint code issues early
        \item Dev teams alerted immediately
    \end{itemize}
    \item Integrated monitoring
    \begin{itemize}
        \item Developers include instrumentation code in app to create metrics data
        \item Metrics dashboards reviewed by team for emerging troublespots
    \end{itemize}
    \item Integrated Security (SecOps)
    \begin{itemize}
        \item Include automated tests on code vulnerabilities, package integrity, valid certificates in CI/CD
    \end{itemize}
\end{itemize}

And the fourth and final benefit is \textbf{consistent deployment environments}
\begin{itemize}
    \item Identical Environments
    \begin{itemize}
        \item Dev/Stage/Prod environments are identical in specification
        \item Avoids `works on my machine!'
        \item Use identical machine configs
    \end{itemize}
    \item Infrastructure as Code (IaC)
    \begin{itemize}
        \item Templating
        \item Config Management
        \item Scripting
    \end{itemize}
\end{itemize}

\subsection{Infrastructure as Code}
The idea behind IaC is to automate the creation, deletion and updates to infrastructure. It also provides consistent repeatability for multiple environments. It enables secure one-use disposable environments. Version controlling infrastructure is also very easy with this. Performing infra-audits is also easy. All of these things are dependent on robust IaaS APIs.

\subsection{Configuration Management}
The use of config management greatly simplifies configuration of multiple servers (or nodes).
\begin{itemize}
    \item Config can be OS patches, apps, websites, scripts, properties ...
    \item Define config state for a node type
    \item CM server/client regularly ensures node state is valid and up to date
\end{itemize}

The config is \textit{idempotent}:
\begin{itemize}
    \item Resources are only allocated once
    \item Manual changes are detected and rolled back
\end{itemize}

Popular CM systems include:
\begin{itemize}
    \item Chef, Puppet, Ansible, SaltStack, DSC
\end{itemize}

IaC often uses \textit{templating}, which is infrastructure defined in either a JSON or YAML template file. The template engine parses file and calls IaaS APIs to build a collection of resources (a `stack'). The most popular IaC templating engines include:
\begin{itemize}
    \item CloudFormation (AWS)
    \begin{itemize}
        \item Has a template section (objects to be created) and several optional sections (including parameters, conditions, mappings, etc.)
    \end{itemize}
    \item Terraform (HashiCorp)
\end{itemize}

\section{Google's Core Cloud Technologies}
We aer going to look at the `under the hood' of Google's underlying infrastructure:
\begin{itemize}
    \item MapReduce (Hadoop)
    \item Google File System (GFS) (HDFS)
    \item Big table (Hbase)
    \item Spanner
    \item F1
\end{itemize}

All of these Google technologies are \textit{proprietary}. The equivalents in brackets are the free equivalent, offered by \textbf{Apache}. The Hadoop software system does not really have any direct equivalent for Spanner or F1.

\subsection{MapReduce}
MapReduce is the key that allowed Google to cluster webpages, and is really the secret behind Google's success. MapReduce is a style of programming and of implementation for generating and processing very large data sets. It's not a specific algorithm, but rather more of an architecture. It was created at Google, and since then around 10,000+ programs have been implemented via MapReduce at Google alone.

It was intended to automatically parallelize `big' data analysis tasks over large clusters built from networks of cheap commodities with some fault-tolerances.

MapReduce partitions the input data into a number of `splits' (the parallelization we mentioned earlier). The size of these splits are defined by the user on creation of the job. There is a \textit{master} in the interim that assigns work to individual machines (called workers). The \textbf{map} part of MapReduce is where some workers are given responsibility for the input files, while the \textbf{reduce} part is where the workers are in charge of the output files.

The master tells the workers on the map side to partition their received data (one of the splits), and this data is going to be read by particular reduce workers.

\begin{enumerate}
    \item First, there is a read operation, where one of the workers \textit{reads} from one of the splits. This is where they extract key/value pairs out of the data
    \item This data is then passed to a \textit{map} function. The map function has been specified by the user, and this creates intermediate k/v pairs; initially stored in a buffer in the worker's memory, before being written to some point in memory.
    \item At this point, the worker feeds back to the master the location of the k/v pair in memory.
    \item The master forwards this data to the reduce workers
    \item This data, among others worked out by other workers, will be read by a reduce worker.
    \item This reduce worker iterates over this new data, and passes each unique key and intermediate values to the user's reduce function (again, provided by the programmer).
    \item The results of this are written to the relevant output file.
\end{enumerate}

This is, of course, repeated for a lot of different workers and for a lot of different output files.

\subsubsection{MapReduce's Hello World}
The equivalent of hello world for MapReduce is counting the number of occurrences of each word in a body of text.

The function might look something like:
\begin{lstlisting}[language=python]
map(String key, String value):
    //key: document name
    //value: document contents
    for each word w in value:
        EmitIntermediate(w,"1");

Reduce(String key, Iterator values):
    //key: a word
    //values: a list of counts
    int result = 0;
    for each v in values:
        results += ParseInt(v);
    Emit(AsString(result));
\end{lstlisting}

\subsubsection{MapReduce Fault Tolerance}
In reality, the instances of MapReduce will have hundreds or thousands of worker machines, so normal failure is a significant issue. To resolve this, a master pings every worker occasionally. If a worker does not respond before a timeout, then the master marks the user as being failed.

Because the results of any map tasks completed by the failed worker, the master resets the failed server's list of map tasks as unallocated and reschedules them onto other map worker machines, replacing the failed machine.

All reduce workers that still need data from the failed map worker are notified of the replacement map worker(s) and so the reducers switch to reading form the local disks of those replacement map workers.

When a map task finishes, it sends a notification to the Master and includes names of the local temporary files where the map outputs are: if the master receives such a notification for a task that has already been completed by a different worker, it ignores it, else it records the completion data.

\subsection{Google File System (GFS)}
When Google started, it committed to building a huge scalable distributed storage capability using cheap commodity components. The components used were cheap and therefore unreliable, so `normal failure' had to be something dealt with by hte file storage system. GFS delivers this.

Files in GFS are divided into fixed-sized 64Mb blocks called \textbf{chunks}. Each chunk is assigned a unique i.d. called a \textbf{chunk handle}. Chunks are stored on \textbf{chunkservers}. Fault tolerance is provided by replicating each chunk across multiple servers. Meta-data needs to be recorded to GFS knows what is where.

GFS consists of three main component types:
\begin{itemize}
    \item GFS Master Server (x1)
    \item GFS Chunk server (multiple)
    \item GFS Client (multiple)
\end{itemize}

\section{BigTable}
BigTable manages large-scale structured data; designed to reliably scale to petabytes of data spread across thousands of machines. BigTable is widely applicable within Google. Workload types vary from batch processing jobs where data-throughput rates are key. It is also the seminal ``NoSQL'' database: ultra large scale, but not fully relational. The locality of the data is under the control of the client.

A BigTable \textit{cluster} is a set of processes running BigTable. Each cluster serves a set of \textit{tables}. Each table is a sparse, distributed, persistent, sorted \textit{map}. The map is from three dimensions (row, column, time) onto a string value (a \textit{cell}). The rows contain data sorted in alpha-order by row key. Rows with consecutive keys are grouped into \textit{tablets}. A table can have any number of columns; the keys of which are grouped into sets called \textit{column families}. Different cells can contain multiple versions of the same data differentiated by a \textit{timestamp}. Garbage collection can keep the most recent $n$ versions of data and/or only keep data that is within $n$ seconds of `now'.

The API of BigTable gives access to functions for:
\begin{itemize}
    \item put/get of table entries
    \item creating/deleting tables and column families
    \item changing metadata associated with clusters, tables, and column families
\end{itemize}

Clients can provide scripts that are executed on BigTable servers: scripting languages called \textit{Sawzall}; client scripts are unable to write back into BigTable, but they can produce filtered, summarized and transformed data from the table. BigTable can be an input source to, or an output target from, MapReduce. The read/write access to disk is via GFS.

\subsubsection{SSTables}
BigTable internally uses \textbf{sorted-string tables} (SSTable), which are an immutable file format, to store data. There isn't much public information on SSTable, but there is an open-source release of this called \textbf{LevelDB}, written in Ruby, that is very similar to how BigTable uses SSTables.

\subsubsection{Chubby/Paxos}
\textbf{Chubby} is a distributed lock service for shared-resource access synchronisation: a system that prioritises availability and reliability rather than high performance. It implements \textbf{Paxos}.

\subsection{Spanner}
Spanner is Google's globally distributed database. It is so called as it `spans' the entire globe; data physically located in any data center anywhere on the planet should be seamlessly accessible and \textit{consistent}. NoSQL approaches like BigTable can store data across multiple datacentres, but ensuring the data is consistent in all locations is very hard. It is an issue due to latency between regions.

Timestamping is used to combat this and is usually done via Network Time Protocol (NTP). NTP can sometimes cause problems, however, such as the subtraction of a leap-second that caused several web services to go down.

For Spanner, Google ignored NTP and developed \textit{TrueTime}. Spanner data-centers are equipped with their own atomic clops, and with GPS receivers to get time reference from GPS signals. The clocks and GPS Rx feed time data to a number of master servers. The masters disseminate time data to all other server machines in the DC. Each server runs a daemon that constantly chekcs with the masters in the local DC and in other DCs to get the time reference signals and to compute this from a consensus view on what the real time is. This means that data can be committed to DCs at multiple difference locations and the common clock means that there is no dispute about which one happened first.

Using Spanner, Google can accurately replicate data across multiple data centres. This gives greater system resilience as any outage can be quickly addressed by firing up the most recent replica or mirror image. It also reduces latency, since if one replica is getting a lot of hits, and a backlog is building up, requests can be diverted to another replica. Spanner automatically shards data across data-centers around the world and automatically reshards data as the amount of data or number of servers changes. Load balancing is done automatically.

The upfront costs for these clocks and GPS technologies are in the thousands of dollars, but spread across the data centers on a per server basis, they are actually quite cheap.

Spanner offers a number of novel features:
\begin{itemize}
    \item Applications given fine-grain dynamic control of data replication configs.
    \item Externally consistent reads and writes
    \begin{itemize}
        \item External consistency guarantees that a transaction will always receive current information. A system is said to provide external consistency if it guarantees that the schedule it will use to process a st of transactions is equivalent to its external schedule.
    \end{itemize}
    \item Globally consistent reads across the database at a timestamp.
\end{itemize}

These features enable Spanner to support a really high scaling system, with things such as consistent backups in bulk.
\end{document}