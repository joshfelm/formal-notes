% Preamble {{{
\documentclass[11pt,a4paper,titlepage,dvipsnames,cmyk]{scrartcl}
\usepackage[english]{babel}
\typearea{12}
% }}}

% Set indentation and line skip for paragraph {{{
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}
\usepackage[margin=2cm]{geometry}
\addtolength{\textheight}{-1in}
\setlength{\headsep}{.5in}
% }}}

\usepackage{hhline}
\usepackage[table]{xcolor}
\usepackage{mathtools}
\usepackage[T1]{fontenc}

% Headers setup {{{
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Information Processing and the Brain}
\rhead{Josh Felmeden}
\usepackage{hyperref}
% }}}

% Listings {{{
\usepackage[]{listings}
\lstset
{
    breaklines=true,
    tabsize=3,
    showstringspaces=false
}

\definecolor{lstgrey}{rgb}{0.05,0.05,0.05}
\usepackage{listings}
\makeatletter
\lstset{language=[Visual]Basic,
backgroundcolor=\color{lstgrey},
frame=single,
xleftmargin=0.7cm,
frame=tlbr, framesep=0.2cm, framerule=0pt,
basicstyle=\lst@ifdisplaystyle\color{white}\footnotesize\ttfamily\else\color{black}\footnotesize\ttfamily\fi,
captionpos=b,
tabsize=2,
keywordstyle=\color{Magenta}\bfseries,
identifierstyle=\color{Cyan},
stringstyle=\color{Yellow},
commentstyle=\color{Gray}\itshape
}
\makeatother
\renewcommand{\familydefault}{\sfdefault}
\newcommand{\specialcell}[2][c]{%
\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}
% }}}

% Other packages {{{
\usepackage{graphicx}
\graphicspath{ {./pics/} }
\usepackage{needspace}
\usepackage{tcolorbox}
\usepackage{soul}
\usepackage{babel,dejavu,helvet}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{tcolorbox}
\usepackage[symbol]{footmisc}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{enumitem}
\setlist{nolistsep}
% }}}

% tcolorbox {{{
\newtcolorbox{blue}[3][] {
colframe = #2!25,
colback = #2!10,
#1,
}

\newtcolorbox{titlebox}[3][] {
colframe = #2!25,
colback = #2!10,
coltitle = #2!20!black,
title = {#3},
fonttitle=\bfseries
#1,
}
% }}}

% Title {{{
\title{Information Processing and the Brain}
\author{Josh Felmeden}
% }}}

\begin{document}
\maketitle
\tableofcontents
\newpage
\section{Information Theory}
Information theory quantifies the amount of information that is potentially available from the communication channel using. So, we look to answer the question: can the receiver decide how informative the channel is likely to be; how much information is in the channel.

\subsection{Randomness}
The key is that if the information in the channel is predictable, the receiver is not going to learn much from this. Therefore, we will look at randomness (coin flipping, geiger counters, etc).

We will also look and unexpectedness, and consider the relationship between randomness, unexpectedness, and information theory.

Netflix is a good example of this. Their star rating of films helps them to recommend films to you, and use your experiences to recommend films to others. The recommended tab is the \textit{channel} in the information theory. The films are the medium, and Netflix is communicating to the user information about the film. If Netflix recommends a really popular film, the information is predictable, and the user is not learning much. Therefore, we are not gaining much information about the quality of the films.

Another example is the star rating of movies. The star rating is not `random' enough for the consumer, in that there is not enough information that we as the user can learn from this. A good film with a lot of hype will likely have a good star rating, and therefore we would already know this.

Therefore, we can say that a statement, despite being correct, can be useless if we can predict it.

\begin{tcolorbox}
\textit{The theory of information starts with an attempt to allow us to quantify the informativeness of information, but not its salience or validity.}
\end{tcolorbox}

\section{Shannon's entropy}
For a finite discrete distribution with a random variable $X$, possible outcomes $\{x_1,, x_2, \dots, x_n\} \exists \mathcal{X}$ and a probability mass function $p_x$ giving probabilities $p_X(x_i)$, the entropy is:
\begin{align*}
H(X) = - \sum_{x_i \exists \mathcal{X}}p_X(x_i) \log_2 p_X(x_i)
\end{align*}

This formula \textit{quantifies} some information received. In the example of star ratings on netflix:

\begin{center}
\begin{tabular}{ll}
    \toprule
    1 star & 0.016 \\
    2 star & 0.310 \\
    3 star & 0.627 \\
    4 star & 0.057 \\ \bottomrule
\end{tabular}
\end{center}

We obtain the following formula:
\begin{align*}
H(X) = -0.016 \log_2 0.016 - 0.31 \log_2 0.31 - 0.627 \log_2 0.627 - 0.057 \log_2 0.057 \approx 1.28
\end{align*}

Choosing base two is arbitrary, but because we are dealing with information, it is useful to deal in bits. In other circumstances, it is possible to choose a different base. The result, $1.28$ is the entropy for the channel of Netflix film star ratings.

If the star ratings were all equally likely, we would get an entropy of:
\begin{align*}
H(X) = -4 \times 0.25 \log_2 0.25 = 2
\end{align*}

This is higher value entropy meaning the information is more useful.

In contrast, if all films are rated one star, we end up with entropy 0, meaning that it is useless. We assume $\log 0$ is 0 for shannon's entropy despite this not being the case in reality.

\subsection{Nice properties}
Shannon's entropy works on any sample space. It's good because you can calculate this on things that would otherwise be impossible, such as items bought from a grocers. This is because it is defined on probability space, while other calculations are calculated in vector space (such as mean, etc).

It's \textbf{always positive}, unless it is zero, which is when the distribution \textit{isn't random} (aka \textit{determined}).

If the distribution is \textit{uniform}:
\begin{align*}
p_X(x_i) = \frac{1}{n}
\end{align*}

for all $x_i$ where ($\#$ is equivalent to \texttt{len})
\begin{align*}
n = \#\mathcal{X}
\end{align*}

then, since $-\log_2(1/n) = \log_2n$
\begin{align*}
H(X) = \log_2n
\end{align*}

We won't prove it here, but is not difficult to prove that:
\begin{align*}
0 \le H(X) \le \log_2n
\end{align*}

with $H(X) = 0$ \textit{only} if one probability is one and the rest zero, and $H(X) = \log_2 n$.

\subsubsection{Case of n = 2}
With two outcomes, $a$ and $b$ with $p(a) = p$ and $p(b) = 1-p$ then
\begin{align*}
H = -p\log_2 p-(1-p)\log_2(1-p)
\end{align*}

The resulting graph of entropy will be symmetric; rising towards 1.

\subsubsection{Source coding}
the main reason to believe that Shannon's entropy is a good quantity for calculating entropy is its relationship to so called source coding.

Consider storing a long sequence of letters \texttt{A, B, C, D} as binary (\texttt{AABACBDA} for example). If we wanted to digitise this, we might use a very simple binary representation of these characters (\texttt{A = 00, B = 01, ...}). Using this representation means the average bits per letter is 2. Now, if we had different distribution of letters, then we could possibly come up with a smaller, more compact version of the code. For example, if we had the following distribution:
\begin{center}
    \begin{tabular}{cccc}
        A & B & C & D \\ \midrule
        0.5 & 0.25 & 0.125 & 0.125
    \end{tabular}
\end{center}
We could use the following encoding:
\begin{center}
    \begin{tabular}{cccc}
        A & B & C & D \\ \midrule
        0 & 10 & 110 & 111
    \end{tabular}
\end{center}

Don't forget that this resulting code should be \textit{prefix free}.

Now, we can prove that this code is shorter on average:
\begin{align*}
L = 0.5 \times 1 + 0.25 \times 2 + 0.125 \times 3 + 0.125 \times 3 = 1.75 < 2
\end{align*}

This is also the same as the Shannon's entropy of the code. Each of the logs are giving us the length of the corresponding code word.

The source coding theorem states that, for the most efficient code, we get:
\begin{align*}
H(X) \le L < H(X) + 1
\end{align*}

This means that if you have a sequence of objects that have some regularity and you want to code them into binary, the channel's entropy is a lower bound on the average length of a message and you can get a \textit{prefix free} code within one of Shannon's entropy.

\subsection{Joint and Conditional Entropy}
Typically, we want to use information theory to study the relationship between two random variables. So far, we have only looked at single variables.

\begin{tcolorbox} [space to upper,
collower=white,
title={Joint Entropy},
nobeforeafter,
halign lower=flush right, ]
Given two random variables $X$ and $Y$, the probability of getting the pair $(x_i, y_j)$ is given by the \textbf{joint probability} $p(X,Y) (x_i,y_j)$. THe joint entropy is just the entropy of the join distribution:
\begin{align*}
H(X,Y) = - \sum_{i,j}p_{X, Y}(x_i, y_j)\log_2p_{X,Y}(x_i,y_j)
\end{align*}

This is essentially the probability that $x$ will produce $x_i$ and that $y$ will produce $y_j$
\end{tcolorbox}

Here's an example:
\begin{center}
    \begin{tabular}{c|cc}
        & $x_0$ & $x_1$ \\ \midrule
        $y_0$ & $1/4$ & $1/4$ \\
        $y_1$ & $1/2$ & $0$
    \end{tabular}
\end{center}

\begin{align*}
H(X,Y) = -\frac{1}{2}\log_2 \frac{1}{4} - \frac{1}{2}\log_2 \frac{1}{2} = \frac{3}{2}
\end{align*}

\subsubsection{Conditional Entropy}
$p_{X|Y}(x_i|x_j)$ is the \textbf{conditional probability} of $x_i$ given $y_j$. If we know that $Y = y_j$, it gives us the probability that the pair is $(x_i, y_j)$.

\begin{align*}
p_{X|Y}(x_i|y_j) = \frac{p_{(X,Y)}(x_i, y_j)}{p_Y(y_j)}
\end{align*}

The \textbf{marginal probability} for $x$ is: $p_X(x_i) = \sum_{j}p_{(X,Y)}(x_i,y_j)$. This is the way of getting the probabilities for one of the two random variables from the joint distribution.

Let's now substitute the conditional probability into the formula for entropy:

\begin{align*}
H(X|Y = y_j) = -\sum_ip_{(X|Y)}(x_i|y_j)\log_2 p_{X|Y}(x_i,y_j)
\end{align*}

This is the entropy of $X$ as we know $Y = y_j$. We call this the \textbf{conditioned entropy}.

The conditional entropy is the average amount of information still in $X$ when we know $Y$. It also has some pretty nice properties:

If $X,Y$ are \textit{independent}, then:
\begin{align*}
    p_{X,Y} (x_i, y_j) = p_X(x_i) p_Y(y_j)
\end{align*} 

For all $i,j$ and
\begin{align*}
p_{X|Y}(x_i|y_j) = p_X(x_i)
\end{align*}

so
\begin{align*}
H(X|Y) = -\sum_{i,j} p_{X,Y}(x_i,y_j) \log_2 p_{X|Y}(x_i|y_j) = H(X)
\end{align*}

Which is what we want, since if $Y$ tells us nothing about $X$, then the conditional entropy should just be the same as the entropy of $X$.

Conversely, if $X$ is \textit{determined} by $Y$, then $H(X|Y) = 0$. This could happen if the only $x_j,y_i$ pairs that actually occur are $(x_i,y_i)$.


\subsection{Mutual Information}
Given two (overlapping) variables, there will be some information that is shared by these two entropies, and this is called \textbf{mutual information}, and is defined as:
\begin{align*}
I(X,Y) = H(X) + H(Y) - H(X,Y)
\end{align*}

Or:
\begin{align*}
I(X,Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
\end{align*}

By substituting in the formulas, we end up with:
\begin{align*}
I(X,Y) = \sum_{i,j} p_{X,Y} (x_i, y_j) \log_2 \frac{p_{X,Y}x_i y_j}{p_X (x_j)p_Y(y_j)}
\end{align*}

Going back to the previous example:
\begin{center}
    \begin{tabular}{c|cc}
        & $x_0$ & $x_1$ \\ \midrule
        $y_0$ & $1/4$ & $1/4$ \\
        $y_1$ & $1/2$ & $0$
    \end{tabular}
\end{center}

This has $H(X,Y) = 3/2, H(X) \approx 0.81$ and $H(Y) = 1$ so $I(X,Y) \approx 0.31$

If $X$ and $Y$ are independent, we just end up with $I(X,Y) = 0$. In fact, $I(X,Y) \ge 0$

\subsection{Correlation}
The correlation is defined as:
\begin{align*}
C(X,Y) = \frac{\langle(X-\mu_X)(Y-\mu_Y)}{\sigma_X \sigma_Y}
\end{align*}

Where $\mu_X$ is the average of $X$ and $\sigma_X$ is the standard deviation.

Correlation only works if $X,Y$ take their values in vector space (meaningfully be able to multiply the two together).

Consider:
\begin{center}
    \begin{tabular}{l|lll}
        & -1 & 0 & 1 \\
        \midrule
        1 & 1/4 & 0 & 1/4 \\
        0 & 0 & 1/2 & 0
    \end{tabular}
\end{center}

Then:
\begin{align*}
C(X,Y) = 0
\end{align*}

Whereas $I(X,Y) = 1$.

\subsection{The data processing inequality}
There is something called \textbf{conditional independence}. Imagine playing a game of snakes and ladders, and $X,Y,Z$ are the resulting position after moves 1,2,3 respectively. Knowing $X$ will change the probability of $Z$, but only if we don't know $Y$, since if we know $Y$, it doesn't really matter what $X$ was. This means that $X$ and $Z$ are conditionally independent (conditioned on $Y$).

The data processing inequality states that if:
\begin{align*}
X \rightarrow Y \rightarrow Z
\end{align*}

then:
\begin{align*}
I(X,Y) \ge I(X,Z)
\end{align*}

With equality if and only if $X \rightarrow Z \rightarrow Y$

\section{Information Theory in the Brain}
\subsection{Information in the Brain}
\textit{Spike trains} are when neurons spike with some level of voltage when they are activated. This spike passes from neuron to neuron, which is how information is passed along an axon.

We can use Shannon's entropy to look at the similarities in these spikes to see the shared information.

An experiment was performed on a fly where it was shown a series of bars on a TV screen and the spike trains from the neurons at the back of the brain (the ones that observe horizontal movement) was recorded. More specifically, the timing of the spikes was recorded. This information was then \textit{discretised}, and evaluated. The information recorded are the action spikes. \textit{Discretisation} meant that the information was looked at in 3ms, and then return a 1 if there is a spike, or a 0 if not (think time bins). This binary sequence was then split up into words. Each word corresponds to the amount of time that an entire piece of information has been encoded. The calculations performed revealed that the neurons that were being looked at only remember things in 30ms, so there was a natural `word length' for splitting up the binary sequence, thus 30ms was taken.

These words can now be converted into a set of objects. The random variable that is taken as the communication channel is a chunk of spike train for 30ms, represented by one of the words. Now, we want to look at the probability of different words:
\begin{align*}
p(w_0) \approx \frac{\#(\text{occurance of } w_0)}{\# (\text{trials})}
\end{align*}

Now, two different tests were performed. The first is where the stimulus is random. The second is where a fixed five minute segment of the stimulus is shown ($H(W)$), and the responses are considered ($H(W|S)$). 

Now, we can get the mutual information about the words given:
\begin{align*}
I(W,S) = H(W) - H(W|S)
\end{align*}

Or, the information about $S$ is the total information in $W$ subtract the noise.

\subsubsection{Discretisation size}
One important question is how to decide how small to make hte discretisation time and how long to make the words. In our example, given that $\delta t = 3ms$ gives $78 \pm 5$ bits per second or $1.8 \pm 0.1$ bits per spike.

Unfortunately, if we have a 30ms word with 3ms letters, this gives us 10 letters per word giving $2^{10} = 1024$. If six seconds of data are used for the repeating stimulus, that is 100 different stimuli, then even for a three hour recording, there are 1800 trails for each stimulus. This is not a big amount for estimating 1024 probabilities.

\subsection{Differential Entropy}
\textbf{Differential Entropy} is the name given to Shannon's entropy for continuous probability distributions where the sample space is $\mathcal{X} \subseteq \mathbf{R}^d$. In the examples we will be looking at, $d = 1$. The idea is that: $h(X) = \int dxp(x) \log_2 p(x)$. This is a good guess as to what the continuous distribution would look like of Shannon's entropy.

If we consider a uniform distribution where:
\begin{align*}
p(x) = \bigg \{ \begin{matrix}
    1/a & x \in [0,a] \\
    0 & \text{otherwise}
\end{matrix}
\end{align*}

So
\begin{align*}
h(X) = \int_{-\infty}^{\infty} dx p(x) \log_2 p(x) = - \frac{1}{a}\int_{0}^{a}dx \log_2 \frac{1}{a}
\end{align*}

And so:
\begin{align*}
h(X) = \log_2 a
\end{align*}

This, unfortunately, does not guarantee that the result will be positive, which is one of the useful properties of Shannon's entropy.

\subsubsection{Gaussian distribution}
$$p(x) = \frac{1}{\sqrt{2 \pi \sigma^2}}e^{\frac{-x^2}{2\sigma^2}}$$

If we substitute and integrate by parts, we get:
$$h(x) = \frac{1}{2} \log_2 2 \pi e \sigma ^2$$

As you expand the distributions, we can prove that for fixed variance, Gaussian has the highest entropy.

Densities are not probabilities. The discrete case $p(x)$ is the probability that $X = x$. For the continuous case:
\begin{align*}
\int_{x_0}^{x_1} dx p(x)
\end{align*}

is the probability that $x_0 \le x \le x_1$. The usual sums and probabilities go to integrals and densities doesn't work because there is a $p$ in the log.

We also need to model the sensitivity of the receiver as well as the behaviour of the source.

\subsection{Relationship between Differential entropy and Shannon's entropy}
Let $\delta = x_{i+1} - x_i$. Consider the discrete random variable $X^\delta$. Now, instead of the continuous distribution we had before, we now have a discrete representation of the interval (differentiation by first principle (by faking it)). A more elegant approach to this is the \textbf{mean value theorem}. This is where you choose two heights for the histogram: the first being the lowest possible point, and the other being the highest, and then choosing the mean point of this.
\end{document}