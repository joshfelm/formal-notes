% Preamble {{{
\documentclass[11pt,a4paper,titlepage,dvipsnames,cmyk]{scrartcl}
\usepackage[english]{babel}
\typearea{12}
% }}}

% Set indentation and line skip for paragraph {{{
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}
\usepackage[margin=2cm]{geometry}
\addtolength{\textheight}{-1in}
\setlength{\headsep}{.5in}
% }}}

\usepackage{hhline}
\usepackage[table]{xcolor}
\usepackage{mathtools}
\usepackage[T1]{fontenc}

% Headers setup {{{
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Information Processing and the Brain}
\rhead{Josh Felmeden}
\usepackage{hyperref}
% }}}

% Listings {{{
\usepackage[]{listings}
\lstset
{
    breaklines=true,
    tabsize=3,
    showstringspaces=false
}

\definecolor{lstgrey}{rgb}{0.05,0.05,0.05}
\usepackage{listings}
\makeatletter
\lstset{language=[Visual]Basic,
backgroundcolor=\color{lstgrey},
frame=single,
xleftmargin=0.7cm,
frame=tlbr, framesep=0.2cm, framerule=0pt,
basicstyle=\lst@ifdisplaystyle\color{white}\footnotesize\ttfamily\else\color{black}\footnotesize\ttfamily\fi,
captionpos=b,
tabsize=2,
keywordstyle=\color{Magenta}\bfseries,
identifierstyle=\color{Cyan},
stringstyle=\color{Yellow},
commentstyle=\color{Gray}\itshape
}
\makeatother
\renewcommand{\familydefault}{\sfdefault}
\newcommand{\specialcell}[2][c]{%
\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}
% }}}

% Other packages {{{
\usepackage{graphicx}
\graphicspath{ {./pics/} }
\usepackage{needspace}
\usepackage{tcolorbox}
\usepackage{soul}
\usepackage{babel,dejavu,helvet}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{tcolorbox}
\usepackage[symbol]{footmisc}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{enumitem}
\setlist{nolistsep}
% }}}

% tcolorbox {{{
\newtcolorbox{blue}[3][] {
colframe = #2!25,
colback = #2!10,
#1,
}

\newtcolorbox{titlebox}[3][] {
colframe = #2!25,
colback = #2!10,
coltitle = #2!20!black,
title = {#3},
fonttitle=\bfseries
#1,
}
% }}}

% Title {{{
\title{Information Processing and the Brain}
\author{Josh Felmeden}
% }}}

\begin{document}
\maketitle
\tableofcontents
\newpage
\section{Information Theory}
Information theory quantifies the amount of information that is potentially available from the communication channel using. So, we look to answer the question: can the receiver decide how informative the channel is likely to be; how much information is in the channel.

\subsection{Randomness}
The key is that if the information in the channel is predictable, the receiver is not going to learn much from this. Therefore, we will look at randomness (coin flipping, geiger counters, etc).

We will also look and unexpectedness, and consider the relationship between randomness, unexpectedness, and information theory.

Netflix is a good example of this. Their star rating of films helps them to recommend films to you, and use your experiences to recommend films to others. The recommended tab is the \textit{channel} in the information theory. The films are the medium, and Netflix is communicating to the user information about the film. If Netflix recommends a really popular film, the information is predictable, and the user is not learning much. Therefore, we are not gaining much information about the quality of the films.

Another example is the star rating of movies. The star rating is not `random' enough for the consumer, in that there is not enough information that we as the user can learn from this. A good film with a lot of hype will likely have a good star rating, and therefore we would already know this.

Therefore, we can say that a statement, despite being correct, can be useless if we can predict it.

\begin{tcolorbox}
\textit{The theory of information starts with an attempt to allow us to quantify the informativeness of information, but not its salience or validity.}
\end{tcolorbox}

\section{Shannon's entropy}
For a finite discrete distribution with a random variable $X$, possible outcomes $\{x_1,, x_2, \dots, x_n\} \exists \mathcal{X}$ and a probability mass function $p_x$ giving probabilities $p_X(x_i)$, the entropy is:
\begin{align*}
H(X) = - \sum_{x_i \exists \mathcal{X}}p_X(x_i) \log_2 p_X(x_i)
\end{align*}

This formula \textit{quantifies} some information received. In the example of star ratings on netflix:

\begin{center}
\begin{tabular}{ll}
    \toprule
    1 star & 0.016 \\
    2 star & 0.310 \\
    3 star & 0.627 \\
    4 star & 0.057 \\ \bottomrule
\end{tabular}
\end{center}

We obtain the following formula:
\begin{align*}
H(X) = -0.016 \log_2 0.016 - 0.31 \log_2 0.31 - 0.627 \log_2 0.627 - 0.057 \log_2 0.057 \approx 1.28
\end{align*}

Choosing base two is arbitrary, but because we are dealing with information, it is useful to deal in bits. In other circumstances, it is possible to choose a different base. The result, $1.28$ is the entropy for the channel of Netflix film star ratings.

If the star ratings were all equally likely, we would get an entropy of:
\begin{align*}
H(X) = -4 \times 0.25 \log_2 0.25 = 2
\end{align*}

This is higher value entropy meaning the information is more useful.

In contrast, if all films are rated one star, we end up with entropy 0, meaning that it is useless. We assume $\log 0$ is 0 for shannon's entropy despite this not being the case in reality.

\subsection{Nice properties}
Shannon's entropy works on any sample space. It's good because you can calculate this on things that would otherwise be impossible, such as items bought from a grocers. This is because it is defined on probability space, while other calculations are calculated in vector space (such as mean, etc).

It's \textbf{always positive}, unless it is zero, which is when the distribution \textit{isn't random} (aka \textit{determined}).

If the distribution is \textit{uniform}:
\begin{align*}
p_X(x_i) = \frac{1}{n}
\end{align*}

for all $x_i$ where ($\#$ is equivalent to \texttt{len})
\begin{align*}
n = \#\mathcal{X}
\end{align*}

then, since $-\log_2(1/n) = \log_2n$
\begin{align*}
H(X) = \log_2n
\end{align*}

We won't prove it here, but is not difficult to prove that:
\begin{align*}
0 \le H(X) \le \log_2n
\end{align*}

with $H(X) = 0$ \textit{only} if one probability is one and the rest zero, and $H(X) = \log_2 n$.

\subsubsection{Case of n = 2}
With two outcomes, $a$ and $b$ with $p(a) = p$ and $p(b) = 1-p$ then
\begin{align*}
H = -p\log_2 p-(1-p)\log_2(1-p)
\end{align*}

The resulting graph of entropy will be symmetric; rising towards 1.

\subsubsection{Source coding}
the main reason to believe that Shannon's entropy is a good quantity for calculating entropy is its relationship to so called source coding.

Consider storing a long sequence of letters \texttt{A, B, C, D} as binary (\texttt{AABACBDA} for example). If we wanted to digitise this, we might use a very simple binary representation of these characters (\texttt{A = 00, B = 01, ...}). Using this representation means the average bits per letter is 2. Now, if we had different distribution of letters, then we could possibly come up with a smaller, more compact version of the code. For example, if we had the following distribution:
\begin{center}
    \begin{tabular}{cccc}
        A & B & C & D \\ \midrule
        0.5 & 0.25 & 0.125 & 0.125
    \end{tabular}
\end{center}
We could use the following encoding:
\begin{center}
    \begin{tabular}{cccc}
        A & B & C & D \\ \midrule
        0 & 10 & 110 & 111
    \end{tabular}
\end{center}

Don't forget that this resulting code should be \textit{prefix free}.

Now, we can prove that this code is shorter on average:
\begin{align*}
L = 0.5 \times 1 + 0.25 \times 2 + 0.125 \times 3 + 0.125 \times 3 = 1.75 < 2
\end{align*}

This is also the same as the Shannon's entropy of the code. Each of the logs are giving us the length of the corresponding code word.

The source coding theorem states that, for the most efficient code, we get:
\begin{align*}
H(X) \le L < H(X) + 1
\end{align*}

This means that if you have a sequence of objects that have some regularity and you want to code them into binary, the channel's entropy is a lower bound on the average length of a message and you can get a \textit{prefix free} code within one of Shannon's entropy.

\subsection{Joint and Conditional Entropy}
Typically, we want to use information theory to study the relationship between two random variables. So far, we have only looked at single variables.

\begin{tcolorbox} [space to upper,
collower=white,
title={Joint Entropy},
nobeforeafter,
halign lower=flush right, ]
Given two random variables $X$ and $Y$, the probability of getting the pair $(x_i, y_j)$ is given by the \textbf{joint probability} $p(X,Y) (x_i,y_j)$. THe joint entropy is just the entropy of the join distribution:
\begin{align*}
H(X,Y) = - \sum_{i,j}p_{X, Y}(x_i, y_j)\log_2p_{X,Y}(x_i,y_j)
\end{align*}

This is essentially the probability that $x$ will produce $x_i$ and that $y$ will produce $y_j$
\end{tcolorbox}

Here's an example:
\begin{center}
    \begin{tabular}{c|cc}
        & $x_0$ & $x_1$ \\ \midrule
        $y_0$ & $1/4$ & $1/4$ \\
        $y_1$ & $1/2$ & $0$
    \end{tabular}
\end{center}

\begin{align*}
H(X,Y) = -\frac{1}{2}\log_2 \frac{1}{4} - \frac{1}{2}\log_2 \frac{1}{2} = \frac{3}{2}
\end{align*}

\subsubsection{Conditional Entropy}
$p_{X|Y}(x_i|x_j)$ is the \textbf{conditional probability} of $x_i$ given $y_j$. If we know that $Y = y_j$, it gives us the probability that the pair is $(x_i, y_j)$.

\begin{align*}
p_{X|Y}(x_i|y_j) = \frac{p_{(X,Y)}(x_i, y_j)}{p_Y(y_j)}
\end{align*}

The \textbf{marginal probability} for $x$ is: $p_X(x_i) = \sum_{j}p_{(X,Y)}(x_i,y_j)$. This is the way of getting the probabilities for one of the two random variables from the joint distribution.

Let's now substitute the conditional probability into the formula for entropy:

\begin{align*}
H(X|Y = y_j) = -\sum_ip_{(X|Y)}(x_i|y_j)\log_2 p_{X|Y}(x_i,y_j)
\end{align*}

This is the entropy of $X$ as we know $Y = y_j$. We call this the \textbf{conditioned entropy}.

The conditional entropy is the average amount of information still in $X$ when we know $Y$. It also has some pretty nice properties:

If $X,Y$ are \textit{independent}, then:
\begin{align*}
    p_{X,Y} (x_i, y_j) = p_X(x_i) p_Y(y_j)
\end{align*} 

For all $i,j$ and
\begin{align*}
p_{X|Y}(x_i|y_j) = p_X(x_i)
\end{align*}

so
\begin{align*}
H(X|Y) = -\sum_{i,j} p_{X,Y}(x_i,y_j) \log_2 p_{X|Y}(x_i|y_j) = H(X)
\end{align*}

Which is what we want, since if $Y$ tells us nothing about $X$, then the conditional entropy should just be the same as the entropy of $X$.

Conversely, if $X$ is \textit{determined} by $Y$, then $H(X|Y) = 0$. This could happen if the only $x_j,y_i$ pairs that actually occur are $(x_i,y_i)$.


\subsection{Mutual Information}
Given two (overlapping) variables, there will be some information that is shared by these two entropies, and this is called \textbf{mutual information}, and is defined as:
\begin{align*}
I(X,Y) = H(X) + H(Y) - H(X,Y)
\end{align*}

Or:
\begin{align*}
I(X,Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
\end{align*}

By substituting in the formulas, we end up with:
\begin{align*}
I(X,Y) = \sum_{i,j} p_{X,Y} (x_i, y_j) \log_2 \frac{p_{X,Y}x_i y_j}{p_X (x_j)p_Y(y_j)}
\end{align*}

Going back to the previous example:
\begin{center}
    \begin{tabular}{c|cc}
        & $x_0$ & $x_1$ \\ \midrule
        $y_0$ & $1/4$ & $1/4$ \\
        $y_1$ & $1/2$ & $0$
    \end{tabular}
\end{center}

This has $H(X,Y) = 3/2, H(X) \approx 0.81$ and $H(Y) = 1$ so $I(X,Y) \approx 0.31$

If $X$ and $Y$ are independent, we just end up with $I(X,Y) = 0$. In fact, $I(X,Y) \ge 0$

\subsection{Correlation}
The correlation is defined as:
\begin{align*}
C(X,Y) = \frac{\langle(X-\mu_X)(Y-\mu_Y)}{\sigma_X \sigma_Y}
\end{align*}

Where $\mu_X$ is the average of $X$ and $\sigma_X$ is the standard deviation.

Correlation only works if $X,Y$ take their values in vector space (meaningfully be able to multiply the two together).

Consider:
\begin{center}
    \begin{tabular}{l|lll}
        & -1 & 0 & 1 \\
        \midrule
        1 & 1/4 & 0 & 1/4 \\
        0 & 0 & 1/2 & 0
    \end{tabular}
\end{center}

Then:
\begin{align*}
C(X,Y) = 0
\end{align*}

Whereas $I(X,Y) = 1$.

\subsection{The data processing inequality}
There is something called \textbf{conditional independence}. Imagine playing a game of snakes and ladders, and $X,Y,Z$ are the resulting position after moves 1,2,3 respectively. Knowing $X$ will change the probability of $Z$, but only if we don't know $Y$, since if we know $Y$, it doesn't really matter what $X$ was. This means that $X$ and $Z$ are conditionally independent (conditioned on $Y$).

The data processing inequality states that if:
\begin{align*}
X \rightarrow Y \rightarrow Z
\end{align*}

then:
\begin{align*}
I(X,Y) \ge I(X,Z)
\end{align*}

With equality if and only if $X \rightarrow Z \rightarrow Y$

\end{document}