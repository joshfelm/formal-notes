% Preamble {{{
\documentclass[11pt,a4paper,titlepage,dvipsnames,cmyk]{scrartcl}
\usepackage[english]{babel}
\typearea{12}
% }}}

% Set indentation and line skip for paragraph {{{
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}
\usepackage[margin=2cm]{geometry}
\addtolength{\textheight}{-1in}
\setlength{\headsep}{.5in}
% }}}

\usepackage{tabularx}
\usepackage{hhline}
\usepackage[table]{xcolor}
\usepackage{mathtools}
\usepackage[T1]{fontenc}

% Headers setup {{{
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Systems and Software Security}
\rhead{Josh Felmeden}
\usepackage{hyperref}
% }}}

% Listings {{{
\usepackage[]{listings}
\lstset
{
    breaklines=true,
    tabsize=3,
    showstringspaces=false
}

\definecolor{lstgrey}{rgb}{0.05,0.05,0.05}
\usepackage{listings}
\makeatletter
\lstset{language=[Visual]Basic,
backgroundcolor=\color{lstgrey},
frame=single,
xleftmargin=0.7cm,
frame=tlbr, framesep=0.2cm, framerule=0pt,
basicstyle=\lst@ifdisplaystyle\color{white}\footnotesize\ttfamily\else\color{black}\footnotesize\ttfamily\fi,
captionpos=b,
tabsize=2,
keywordstyle=\color{Magenta}\bfseries,
identifierstyle=\color{Cyan},
stringstyle=\color{Yellow},
commentstyle=\color{Gray}\itshape
}
\makeatother
\renewcommand{\familydefault}{\sfdefault}
\newcommand{\specialcell}[2][c]{%
\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}
% }}}

% Other packages {{{
\usepackage{graphicx}
\graphicspath{ {./pics/} }
\usepackage{needspace}
\usepackage{tcolorbox}
\usepackage{soul}
\usepackage{babel,dejavu,helvet}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{tcolorbox}
\usepackage[symbol]{footmisc}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{enumitem}
\setlist{nolistsep}
% }}}

% tcolorbox {{{
\newtcolorbox{blue}[3][] {
colframe = #2!25,
colback = #2!10,
#1,
}

\newtcolorbox{titlebox}[3][] {
colframe = #2!25,
colback = #2!10,
coltitle = #2!20!black,
title = {#3},
fonttitle=\bfseries
#1,
}
% }}}

% Title {{{
\title{Systems and Software Security}
\author{Josh Felmeden}
% }}}

\begin{document}
\maketitle
\tableofcontents
\newpage
\section{Overview}
We learn about this topic so that we can avoid our own software having these same exploits.

So, what is a program?
\begin{itemize}
    \item Functional (intended) behaviour
    \item Security policy (what it's not meant to do)
\end{itemize}

Unintended behaviours can include:
\begin{itemize}
    \item Design flaws
    \item Bugs
    \item Lower-level bugs
    \item Mistaken assumptions
\end{itemize}

\subsection{Weaknesses and Vulnerabilities}
A \textbf{weakness} is when a program has a flaw that allows an attacker to do something the programmer didn't anticipate, or which could cause problems.

A \textbf{vulnerability} is when these weaknesses can be \textit{exploited} by an attacker to violate part of the program's design and do something harmful.

\begin{tcolorbox}
    \begin{center}
        Weakness is \textit{not} a vulnerability
    \end{center}
\end{tcolorbox}

Just because a program has a weakness does not mean it is exploitable.

An \textbf{exploit} is a program or technique that takes advantage of a vulnerability to violate the security policy. They can be published to prove existence of a vulnerability or utilised as part of malware.

\begin{itemize}
    \item High-level code gets translated into a low-level representation
    \item Separate variables become continuous memory addresses
    \item Data types become bit-patterns
    \item Memory corruption becomes a big problem
\end{itemize}

And typical vulnerabilities we will see are:
\begin{itemize}
    \item Over/underflow
    \item Data corruption
    \item Control flow corruption
    \item Denial of service
\end{itemize}

These normally cause the program to crash, but occasionally they can become an \textit{exploit} where we can gain access to places we shouldn't have.

\subsection{Mitigations}
We can put in place mechanisms that remedy the weakness, or prevent the exploitation of the vulnerability. For example, stack canaries let us spot when a stack buffer has overflown. Note that it doesn't fix the buffer overflow but it makes it a \textbf{lot harder} to exploit. We can also randomise where memory is kept (ASLR), shadow stacks, sandboxing (such as a firewall).

\subsection{The C programming language}
We will mostly be looking at C in this module because it's a really popular programming language. It's not dead, honest!!! Also, pretty much everything is built on top of it.

It's designed for systems programming and is unsafe \textit{by design}. It is therefore the programmers job to ensure that their program is correct, allowing the programmer to access raw(ish) memory addresses (pointers).

People don't like C (me included) because it always assumes the programmer knows best. It has limited support for anything more than primitive types, and even some primitive types have limited support. It also has limited bounds settings and setting a variable to \texttt{const} doesn't actually make it a constant, because you can still edit the variable if you know where it's stored in memory.

It's not all bad, though. A lot of legacy code is still written in C. Some effort has been made to rewrite this code in safer languages, but this isn't always possible or even a good idea. While C is very stable and portable and really useful, it can lead to bugs (though not all bugs relate to Cs unsafeness, some of it could be the programmer being a dummy). Rewriting C could lead to whole new bugs and oversights.

\subsection{Assembly}

\subsubsection{Memory Layout}
While we can generalise, it is important to note that not all memory looks the same. Different architectures and OSs might have memory look different.

From low to high:
\begin{itemize}
    \item .text (Program code)
    \item .plt (Library code)
    \item .data (initialised data)
    \item .bss (uninitialised Data)
    \item The heap (growing up)
\end{itemize}

From high to low:
\begin{itemize}
    \item Arguments and environment
    \item The stack (growing down)
\end{itemize}

\begin{tcolorbox}
\textbf{NOTE:} Stack goes down, heap goes up.
\end{tcolorbox}

\subsubsection{x86 Assembly (32-bit)}
There are 6 32-bit general purpose general registers: \texttt{eax, ebx, ecx, edx, esi, edi}, 2 special 32-bit registers: \texttt{esp, ebp} and 1 instruction pointer: \texttt{eip}. There are sometimes more registers depending on the chip and also tonnes of instructions, since there's a pretty big CISC (this normally gets translated into a RISC microcode, but not always)

\subsubsection{amd64 Assembly}
There are 16 64-bit general purpose registers: \texttt{raz, rbx, rcx, rdx, rsi, rdi, r8, r9, r10, r11, r12, r13, r14, r15}, 2 special 32-bit registers: \texttt{rsp, rbp} and 1 instruction pointer \texttt{rip}. Again, there can sometimes be more registers depending on the chip and heaps of instructions (which NORMALLY get translated into RISC but sometimes doesn't. Look at the manual if you want to know CHRIST).

\subsubsection{x86/64 Assembly}
There are lots of different assemblers for x86, each with their own syntax. There are strong opinions about what is better, but you need to kind of get a feel for what works for you.

\subsection{Calling Conventions}
Calling conventions handle how functions are called from C, translation of this into registers, where arguments go for shared libraries, etc.

It's defined by the OS but not strictly enforced. Most programming languages follow the rules set by C.

There are a lot of different x86 calling conventions, and you pretty much just have to look up whatever your system uses (Windows uses more than one, helpfully).

In essence:
\begin{itemize}
    \item \texttt{cdecl}: everything goes on the stack, caller cleans up
    \item \texttt{stdcall}: everything goes on the stack, callee cleans up
    \item \texttt{fastcall}: pass things in registers \texttt{eax, edx, ecx} then on the stack
    \item \texttt{thiscall}: class pointer in \texttt{exc} then stack (usually for c++ or Windows)
\end{itemize}

\subsubsection{amd64 Calling conventions}
With amd64, the instruction set designers sorted a lot of the mess out and started again. Now, we only have two (kind of three) conventions, similar to fastcall. Again, look it up.

\subsection{Useful Tools}
\begin{itemize}
    \item Debuggers: \textbf{GDB} or LLDB
    \item Disassemblers: Ghidra, Radare2, Objdump
    \item Languages: Python
    \item Hex Editors: Radare2, XXD, emacs???/vi
\end{itemize}

\textbf{Compilation Options}
\begin{itemize}
    \item For GCC
    \begin{itemize}
        \item \texttt{-fno-stack-protector}
        \item \texttt{-z execstack} (run shellcode off the stack)
        \item \texttt{-mno-accumulate-outgoing-args} (don't optimise calling conventions)
    \end{itemize}
\end{itemize}

\section{Software Vulnerabilities and Attacks Part 1}
\subsection{Buffer Overflows}
When you declare an array in C, you get a region of memory. Pointers are used to address arrays, and it's very easy to fall off the edge of this region. They have been known about since the dawn of computers, so it's nothing new.

To understand buffer overflows, we need to understand how functions work. We write from the top of the stack to the bottom of the stack. So, when we go into a function, we push the memory address of the stack before the function call onto the stack. Once we've done that, we push the variables of the function onto the stack. This is the basic idea for memory layout for stacks.

Now, what if it got a little more interesting?:
\begin{lstlisting}[language=C]
//example 1.c
void function(char *str) {
    char buffer[16];
    
    strcpy(buffer,str)
}

void main() {
    char large_string[256];
    int i;

    for(i = 0; i < 255; i++) {
        large_string[i] = 'A';
    }

    function(large_string);
}
\end{lstlisting}

The memory layout would look like this:
\begin{lstlisting}
TOS                                                 BOS
            buffer          sfp     ret  *str
<-------    [AAAAAAAAAAAAA][AAAAA][AAAAAA][AAAAAAAAAAAAAAA...]
\end{lstlisting}

Since \texttt{strcpy} only deals with pointers, we just start writing 'A' into the buffer, and once it reaches the end of the buffer, it just keeps writing. Now, once the function is finished, it returns. When it attempts to read the memory address for the return, it's going to try to return to `AAAAAA', which will probably crash the function.

Being able to overwrite stack data is bad, but overwriting return addresses gives us arbitrary code execution. Normally, it just causes an access validation, or a bad instruction. But, sometimes, you can take over the program.

\subsubsection{shellcode}
The classic way of doing this is with buffer shellcode. This rarely works now, but you can turn off the protectors that stop this happening. THe modern way of doing this is \textit{return oriented programming (ROP)} and we'll visit this later.

There are some tricks to make it easier:
\begin{itemize}
    \item Alphabetic shellcode
    \item NOP-sleds (instructions that do nothing, padding the addresses)
\end{itemize}

\subsubsection{Prevention methods}
\begin{itemize}
    \item Stack canaries spot if buffers have been overrun.
    \item W\^X (write xor execute) makes shellcode harder (but not impossible)
    \item Use bounded data structures and not the old C ones
    \item Use the bounded memory functions (\texttt{strncpy})
    \item Use a modern compiler toolchain and turn on all the security features
\end{itemize}

\subsection{Format Strings}
A format string is a vulnerability in C-style print functions. It allows an attacker to read from the stack and other places. It also allows an attacker to write to any memory addresses on the stack.

With \texttt{printf}, if we don't put enough arguments, such as \texttt{printf("Hello \%s! \textbackslash n)}, we would get a warning, because the compiler can't be sure that it is wrong.

If we then combine this with something like \texttt{gets}, we are able to access the stack arbitrarily, and even write to it with \texttt{\%n}

To fix this, we can just listen to the compiler warnings. Some modern systems remove the functionality with it, while others log its use.

\subsection{Race Conditions}
Computers can do more than one thing at once, and sometimes the order gets messed up which can lead to bugs. Here's a really simple increment function:
\begin{lstlisting}
void increment(int *n) {
    int temp;

    temp = *n;
    temp += 1;
    *n = temp;
}
\end{lstlisting}

This isn't thread safe, however, because if we are not careful we can lose increments. If two users call this really quickly, we might lose one of the increments. This, at the moment, is only a correctness issue. How does it become a security issue?

The \textit{access} system call checks the accessibility of the file named by the path argument for the access permissions indicated by the mode argument. If we have a suid-program that does controlled writes as root, then it checks using access if your real user can write to a file, then does the writing as root. To avoid this kind of race condition, we can just use synchronisation around time-of-check and time-of-use. These kinds of bugs are really quite dangerous and hard to deal with, though.

\section{OS Security}
\subsection{What is an OS?}
An operating system provides an abstraction over the computer's hardware. Bigger OSs have to run more than one program with more than one user. We normally like it to implement some security policies.

\textbf{Access Control Security Goals} are essentially:
\begin{itemize}
    \item \textbf{Confidentiality}: you can't see what you don't need to see
    \item \textbf{Integrity}: you can't tamper with stuff that's not yours
    \item \textbf{Availability}: you can get at your stuff.
\end{itemize}

These goals are interdependent: if I can tamper with data, who cares if it is confidential?

A \textbf{principal} is a person describing the access control policy or human trying to follow the policy.

\textbf{Object} is the resources that we are writing the policy about.

\textbf{Subjects} are the things (processes) interacting with the objects that we are trying to restrict.

\subsubsection{UNIX DAC --- Discrentionary Access Controls}
This is the traditional access control mechanism present in almost all OSs in some form. Objects have an owner and a group. At the owner's \textit{discretion}, they can say what they, the group, and everyone else can do with the object (read, write, execute).

There are some flaws with DAC, unfortunately. Imagine a user (Alice) wants to run a web browser. We would like that to be able to access her downloads folder, but probably not the SSH keys.

Now, imagine Alice wants to run an SSH server. We want her to be able to access her SSH keys but probably not the downloads folder.

In essence, the DAC policy is described at the object level. We could work around it, so Alice's programs run as an Alice-unprivileged user and use the group permissions to set where they can access, and then duplicate the policy for multiple users, so this isn't really viable since it gets so complicated really fast, as well as being hard to verify. This doesn't mean it's impossible, and some systems do utilise this.

The other problem is that do we trust the sysadmin to get the policy right? We need a mechanism to be able to enforce a security policy from the top down, and not just rely on discretionary controls. This is the \textit{principle of least privilege}.

\subsubsection{Reference Monitors}
These reference monitors are going to help us fix this dilemma. We will still have \textit{subjects}, but processes are associated with a security context (user, group, privileges). We will also still have \textit{objects}, and these will have security information (DAC and xattrs (extra attributes)).

On login, processes get the capabilities of their principal, and then these are progressively dropped. Processes also inherit the capabilities of the process that made them.

There is no way for subjects to access objects except through the reference monitor (complete mediation). When a subject makes a system call:
\begin{itemize}
    \item Get information about the subject
    \item Get information about the object
    \item Apply the system policy based on the information
    \item Log that a decision was made
    \item Return the decision
\end{itemize}

Race conditions can crop up in this, so be aware of that.

\subsubsection{MAC --- Mandatory Access Controls}
The sysadmin sets the access control policy (which may just be the DAC). The simplest form in \textit{multi level security}, which emerged from the US military. Subjects and objects associated with a security level:
\begin{itemize}
    \item Unclassified
    \item Confidential
    \item Secret
    \item Top secret
\end{itemize}

This is the usual hierarchy of levels, but might have more or less.

One security model is the \textbf{Bell-LaPadula} method, meaning people can't read above their security clearance, or write to a security level lower than their level (don't want someone accidentally leaking data to lower levels). This method focuses on \textit{confidentiality}.

Another model is the \textbf{Biba} method. This is a no read-down, no write-up. This preserves \textit{integrity}.

\subsection{Linux Security Modules}
The solution to Linux's security is the Linux Security Model (LSM) framework. This implements a reference monitor for Linux. It has dynamically loadable kernel module hooks into system call checks. The framework is verified, and modules are (in theory) small and verifiable. The hook function returns access descision:
\begin{itemize}
    \item \texttt{0}: Access granted
    \item \texttt{ENOMEM}: No memory available
    \item \texttt{EPERM}: Not enough privileges.
\end{itemize}

\subsubsection{SELinux}
One we are going to look at in detail is \textbf{SELinux}. This is a framework originally developed by the NSA. It's based around type-based enforcement and RBAC (role based access controls).

The types of hooks possible are:
\begin{itemize}
    \item Management hooks
    \begin{itemize}
        \item Called to handle object lifecycle
    \end{itemize}
    \item Path-based hooks
    \begin{itemize}
        \item Related to pathnames
    \end{itemize}
    \item Object-based hooks 
    \begin{itemize}
        \item Path kernel structure corresponding to objects
    \end{itemize}
\end{itemize}

Need a mechanism to interact with SELinux from userland. This enables the filesystem to load policies and configuration. It also gets audit data.

All subjects get labeled with a security context:
\begin{itemize}
    \item User
    \item Domain
    \item Role
\end{itemize}

Rules describe what each \textit{subject} domain can do with an \textit{object} domain. They can get a bit complicated. An example of this is \texttt{/etc/passwd}, where the user information readable by any user. Or \texttt{/etc/shadow} password information readable by root only. The way this is done looks like this:
\begin{lstlisting}
'normal users are allowed to read normal files
allo user_t public_t : file read

'users in the password_t domain can r/w files in the password_data_t domain
allow passwd_t passwd_data_t : file {read write}

'allow users to actually run the password program, and transition their domain
allow user_t passwd_exec_t : file execute
allow user_t passwd_t : process transition
type transition user_t passwd _exec_t : process passwd_t
\end{lstlisting}

This seems very complicated, but it kind of makes sense. The rule design is very hard.

\subsection{Intrusion detection}
Intrusion detection is a service that monitors a system and looks for unusual or failed attempts to access system resources:
\begin{itemize}
    \item Could be a single event, could be a combination
    \item Could be probabilistic
    \item Could be running on the host
    \item Could be running on the network.
    \item Usually attempting to do detection in real-time (or near)
\end{itemize}

We are looking for failed authentication attempts or odd network traffic. Another thing we are looking for is users running unusual processes, or accessing unusual files. Essentially, anything unusual should be flagged.

Here are some types of intrusion detection systems
\begin{itemize}
    \item Host based
    \begin{itemize}
        \item Runs as a privileged process on the host
        \item Uses information from the OS/reference monitor
    \end{itemize}
    \item Network based
    \begin{itemize}
        \item Runs either on the host or on the networ
        \item Looks at network traffic, who is contacting who and how often
    \end{itemize}
    \item Signature based
    \begin{itemize}
        \item Identify attacks based on known attack patterns
    \end{itemize}
    \item Anomaly based
    \begin{itemize}
        \item Identify attacks based on a machine-learning model of what is normal for a given user or process
    \end{itemize}
\end{itemize}

False positives or negatives are really annoying but they can be fed into the rules for next time.

The goals of the IDS are to run continuously and resist attempts to subvert mechanisms. It shouldn't make the system unusable in terms of performance or usability overhead. It should adapt to changes in a system's use and reconfiguration. It should also scale to work with big systems. It should degrade gracefully and not fail (ideally).

\section{Software Vulnerabilities and Attacks 2}
\subsection{Heap overflow}
Heap based overflows involve a discussion of \texttt{malloc}, unfortunately. This is \textit{very} system dependent and has changed a lot over time. Therefore, we are going to go \textit{high-level} and describe the concepts and history. To understand in detail, the implementation of the system must be researched.

While we normally allocate memory with \texttt{malloc} and \texttt{free}, they are actually using something called \texttt{mmap} (memory map). This command works via the kernel to assign and manage regions of memory. But, system calls are expensive and creating or new-ing objects dynamically is really common. C is supposed to be pretty portable, and since not all OSs implement POSIX APIs portably, we instead manage memory via the user, as opposed to the kernel.

When a program starts, we give it a large region of memory somewhere in its virtual address space and an API for managing it. It can call the lower-level system calls if necessary. Data structures to manage things were initially based on a heap, so let's call this the heap and we keep it as far away as possible to avoid things bumping into each other.

So, \texttt{malloc} and \texttt{free} are the libC API for dynamically assigning memory for objects. The essential idea is:
\begin{itemize}
    \item Ask for memory with \texttt{malloc/calloc}
    \item Mark it as used with free
    \item Dynamically grow or change with \texttt{realloc}
\end{itemize}

Heap overflows are kind of not realistic, so we will look at Glibc \texttt{malloc}.

\subsubsection{Glibc Malloc}
Memory starts out as a big empty array (called an arena). When malloc is called, put the following chunk data structure on the heap. Return the pointer to the start of the payload. Data gets more and more chunked as time goes on. On free, write some data into the old payload, including a pointer to the next chunk forward and a pointer to the last point back. There are various sizes, but sequential. When freeing memory, check the forward and back pointer, if the previous chunk is also freed, then merge the two chunks together and update the length to be combined (headers).

We can attack this via making a chunk that looks like it's already been freed. We can set headers in our own tables, since we know that the size field will be added to the address before a pointer you control. If we manage to do an arbitrary write (return address) we can completely compromise the system. This is a lot of work for a single integer write, but sometimes this is all you need.

\subsection{Return oriented Programming}
We looked at \textit{smashing the stack} earlier, as well as \textit{injecting shellcode}. But, this doesn't really work anymore, since OSs mark memory sections as marked, so injecting shellcode is a thing of the past (since the 90s really). But, why do we need to write a program into memory at all?

Shellcode itself simply sets up registers, pushes the location of the shell to the stack, gets the stack pointer, and then calls \texttt{execve}. There is already a command for doing this in C, however, called \texttt{system()}. This function:
\begin{itemize}
    \item Runs a program in the system shell
    \item so there \textit{must} be a \texttt{/bin/sh} string already in memory to pass to the exec syscall
    \item If we know its address, do we even need to push it on?
\end{itemize}

The basic idea is that instead of injecting the shellcode, we can set up the stack so that it looks like the arguments to a call to \texttt{system()} and assume the cdecl calling convention. Therefore, instead of returning onto our shellcode, we'll return into the libc \texttt{system()} function.
To do this, we need a few things:
\begin{itemize}
    \item System needs to be already loaded into memory
    \item ASLR can be problematic, but depends on how its implemented
\end{itemize}

Unfortunately for the attacker, the fixes for this are pretty easy. AMD64 architecture doesn't pass arguments on the stack by default. If more randomisation is added, it's harder to guess library functions. Also, ASCII armour strings are in memory by XORing them with patterns to make them harder to steal.

Remember turing machines from second year? It turns out that if you make it in a certain way, you can make universal computation. So, wouldn't it be really unfortunate if you could make a Turing machine out of the instructions right before the return instruction in a program's memory?

Turns out, you can do this, and this is called \textbf{Return Oriented Programming (rop)}. We know a buffer overflow gives us control over the stack, instead of overwriting just \textit{one} return address, we can write a series of stack frames. Each saved instruction pointer will be to an instruction just before a return instruction. This is called \textbf{gadgets}. Instead of writing shellcode, we find a series of gadgets that when run in sequence, have the same effect. If we manage to find a set of gadgets that is Turing complete, we can reuse the existing program code to implement ANY shellcode without injecting the actual shellcode.

\newpage
Right then, whats the plan of attack?
\begin{enumerate}
\item Find a gadget for `\texttt{pop rdi' ret}'
\item[N-1.] Setup stack as \texttt{\&(pop rdi' ret) | \&("/bin/sh") | \&system}
\item[N.] Return
\end{enumerate}

We also need to defeat ASLR, since libraries usually get dynamically loaded into memory by mmaping the whole library into memory. If we can leak a pointer of something within the pointers where all the functions are in a library (held in the .got file), we can learn where the pointers are. So, new plan of attack:
\begin{enumerate}
\item Find a gadget for `\texttt{pop rdi; ret}'
\item Find .got entry for the puts function
\item Leak it
\item Recall main (so we don't randomise memory)
\item Calculate libc's ASLR offset and where memory addresses really are
\item Setup stack as \texttt{\&(pop rdi' ret) | \&("/bin/sh") | \&system}
\item Return
\end{enumerate}

\section{Software Defence}
\subsection{Program Analysis}
We need to ensure that the software development lifecycle is secure. It's not easy to find bugs manually in large scale programs.

Manual testing can only go so far. If we have loops, it can become quite difficult to see all of the possible execution paths, even in very small programs. Static analysis, therefore, is very difficult.

Program analysis is the automatic process of analysing the behaviour of computer programs regarding a property such as correctness, reliability, safety, and security. The types we will see is:
\begin{itemize}
    \item Static Analysis: performed without executing the program
    \item Dynamic Analysis: performed at runtime
    \item Hybrid: a mix of the two
\end{itemize}

\subsubsection{Static Program Analysis}
This method of analysis is just looking at the code without executing it. This becomes really quite difficult with large scale programs. For things like memory allocation, you might not even know how that will work (random analysis etc.). Binary code is even more challenging because actually understanding it would take such a long time. Compilers make heavy use of this analysis to ensure correctness of programs.

This is beneficial because we can analyse every component and path of the application if we have access to the code.


The tools we have access to are:
\begin{itemize}
    \item LLVM
    \item For binary code we have a few: IDA, Ghidra, Miasm, angr...
\end{itemize}

\subsubsection{Dynamic Progam Analysis}
This method of analysis is essentially like debugging --- it is analysed at runtime. Dynamic analysis is both very precise and scales very well. However, it is limited to the executed code of the program, so coverage is a problem.

The benefits are that we can look at things like the dynamic allocation of memory and profiling and so on.

The tools we have access to are:
\begin{itemize}
    \item Intel pin, Dyninst, Valgrind
    \item Security tools (...)
\end{itemize}

\subsubsection{Soundness and Completeness}
We have two separate conditions we need to check for when analysing a program: \textbf{soundness} and \textbf{correctness}.

\begin{itemize}
    \item \textit{Soundness} is essentially saying that if analysis says no bugs, there are no bugs and vice versa. {\color{blue}If analysis says true $\rightarrow$ true}.
    \item \textit{Completeness} essentially states that if there are no bugs, analysis will say there are no bugs. {\color{blue}True $\rightarrow$ analysis says True}.
\end{itemize}

\begin{center}
\begin{tabular}{c|c|c}
   & \textbf{Complete} & \textbf{Incomplete} \\ \hline
   \textbf{Sound} &
   \begin{minipage}[t]{0.4\textwidth}
   \begin{itemize}
   \item Reports all errors
   \item Reports no false alarms
   \end{itemize}
   \centering
   {\color{red}\textbf{Undecidable}}
   \end{minipage}
   &
   \begin{minipage}[t]{0.4\textwidth}
   \begin{itemize}
   \item Reports all errors
   \item May report false alarms
   \end{itemize}
   \centering
   {\color{green}\textbf{Decidable}}
   \end{minipage}
   \\ \hline
   \textbf{Unsound} &
   \begin{minipage}[t]{0.4\textwidth}
   \begin{itemize}
   \item May not report all errors
   \item Reports no false alarms
   \end{itemize}
   \centering
   {\color{green}\textbf{Decidable}}
   \end{minipage}
   &
   \begin{minipage}[t]{0.4\textwidth}
   \begin{itemize}
   \item May not report all errors
   \item May report false alarms
   \end{itemize}
   \centering
   {\color{green}\textbf{Decidable}}
   \end{minipage}
\end{tabular}
\end{center}

In most cases, the program will be both unsound and incomplete.

\subsubsection{Generic Approach}
We need to decide whether our program is intraprocedural or interprocedural:
\begin{itemize}
    \item Intra --- per function analysis (ignoring side effect of function calls)
    \item Inter --- Function analysis (considering side effects of function calls)
\end{itemize}

Next, we generate a control flow graph (CFG), and then optionally generate the interprocedural CFG (ICFG) and finally we data-flow analyse the generated CFGs.

\subsubsection{Intermediate Representation}
We can represent each complex statement that we have via `high-level' assembly code. This will contain:
\begin{itemize}
    \item Binary logic and arithmetic operators
    \item Use of temporary memory locations
    \item Assignment to variables, temporary locations
    \item A label assigned to each instruction
\end{itemize}

E.g.:
\begin{lstlisting}
var1 = (var2 + var3)  + func(A)
' Translates to
L1: t1 = var2 + var3
L2: t2 = func(A)
L3: var1 = t1+t2
\end{lstlisting}

As shown, it is called 3-address code because we only use 3 memory addresses. This is maintained by creating new temporary variables (in this example, \texttt{t1} and \texttt{t2}).


\subsubsection{Basic Block (BB)}
A maximal sequence of instructions with single entry and exit. Execution of BB is \textit{atomic} under normal conditions.

\subsubsection{Control Flow Graph}
Control flow graphs are a representation of how the execution may progress inside a given function It is a graph $(V,E)$ such that:
\begin{itemize}
    \item $V = \{B_i\}$ where $B_i$ is a basic block.
    \item $E = \{B_i,B_j\}$ where the last instance of $B_i$ is a jump to the first instance of $B_j$ and the first instance of $B_j$ follows the last instance of $B_i$ in the TAC.
\end{itemize}

\subsubsection{Call Graph}
A call graph is computed for the whole program and is represented as a directed graph $(V,E)$ such that:
\begin{itemize}
    \item $V = \{F_i\}$ where $F_i$ is a function
    \item $E = \{(F_i,F_j)\}$ where $F_i$ calls $F_j$.
\end{itemize}

\subsubsection{Redundant Expressions}
An expression is redundant at a location if:
\begin{itemize}
    \item It is computed at location $i$
    \item This expression is computed on every path going from initial location to location $i$
    \item On each of these paths, operands of $e$ are not modified between the last computation of $e$ and location $i$.
\end{itemize}

Optimisation is performed as follows:
\begin{itemize}
    \item Computation of the available expressions (via data flow analysis)
    \item $x:=e$ is redundant at location $i$ if $e$ is available at $i$
    \item $x:=e$ is replaced by $x:=t$ (where $t$ is a temp memory address containing the value of $e$).
\end{itemize}

We then look at the variables to see if they are changing to evaluate whether they are redundant or not.

\subsubsection{Dataflow Equation for Available Expressions}
For a basic block $b$ we note:
\begin{itemize}
    \item $In(b)$: available expressions when entering $b$
    \item $Kill(b)$: expressions made \textit{non-available} by $b$ (because an operand of $e$ is modified by $b$)
    \item $Gen(b)$: expressions made \textit{available} by $b$ (computed in $b$ and operands not modified afterwards)
    \item $Out(b)$ available expressions when exiting $b$
\end{itemize}

\begin{align*}
Out(b) = (In(b) \backslash Kill(b)) \cup Gen(b) = F_b(In(b))
\end{align*}

Where $F_b$ is a \textbf{transfer function} of block $b$.

To compute $In(b)$:
\begin{itemize}
    \item If $b$ is the initial block:
    \begin{align*}
    In(b) = \emptyset
    \end{align*}
    \item If $b$ is not the initial block, an expression $e$ is available at its entry point iff it is available at the exit point of {\color{red}\textit{each}} predecessor of $b$ in the CFG
    \begin{align*}
    In(b) = \bigcap_{b' \in Pre(b)} Out(b')
    \end{align*}
\end{itemize}

This is called forward data-flow analysis along the CFG paths.

\subsubsection{Reaching Definition}
Every assignment is a definition. A definition $d$ reaches a point $p$ if there exists a path from the point immediately following $d$ to $p$ such that $d$ is not killed (overwritten) along that path. 
\subsection{Dynamic Analysis}
We have already looked at the static version of analysis, and this is useful, but it leaves some gaps in the space that we have not analysed. As previously discussed, there may be some functions that are never statically referenced in previously visited code. The solution to this is to run dynamic analysis. The program should be run multiple times and observe the targets of indirect code jumps and calls.

\textbf{Dynamic analysis} is a technique that is performed at runtime and has historically been used for performance monitoring and software testing. Security related behavioural analysis is all based on dynamic analysis.

We can monitor call instructions at runtime using \textbf{GDB}. We can set a breakpoint at each function call before the program starts and then look at the stack. But, this is a lot of effort. Instead, we can use the binary to log the targets of all indirect call or jump instructions for us automatically.

\subsubsection{Instrumentation}
This is a technique that injects instrumentation code into a binary to collect run-time information. It might just inject some \texttt{printf} functions to say when a function is entered, or inject things like a loop counter that is output when the loop increments. NOTE: it does not modify the semantics of the program.

Instrumentation is useful when we are profiling for compiler optimisation or performance profiling. It is also useful for Bug detection or vulnerability identification or even exploit generation. It is also used for architectural research, using both processor and cache simulation, and trace collection.

\textbf{Static instrumentation} is instrumentation performed before runtime. This comes in three flavours:
\begin{itemize}
    \item \textbf{Source code instrumentation} --- instrument source programs
    \item \textbf{IR instrumentation} --- instrument compiler-generated (like LLVM)
    \item \textbf{Binary instrumentation} --- Instrument executables directly by inserting additional assembly instructions.
\end{itemize}

\textbf{Dynamic binary instrumentation} is performed just after runtime (just in time --- JIT). We use binary instrumentation because libraries are a big pain for source or IR level instrumentation. It also easily handles multi-lingual programs. Additionally, worms and viruses are rarely provided with source code.

\begin{itemize}
    {\color{green}
    \item Pros:
    \begin{itemize}
        \item No need to recompile or relink
        \item Discovers code at runtime
        \item Handles dynamically generated code
        \item Attaches to running processes
    \end{itemize}
    }
    {\color{red}
    \item Cons:
    \begin{itemize}
        \item Usually higher performance overhead
        \item Requires a framework which can be detected by malware
    \end{itemize}
    }
\end{itemize}

\subsubsection{Pin}
Intel Pin is a dynamic binary instrumentation tool. It supports both Linux and Windows executables for x86, x86\_64 and IA-64 architectures. It allows a tool to insert arbitrary code in arbitrary places in the executable while the executable is running. This also makes it possible to attach pin to already running processes.

Pin allows the full examination of any x86 instruction, tracking function calls (including library and sys calls), and tracking application threads, among others.

\subsection{Fuzzing}
Fuzzing is a type of analysis. It's not like fuzzy matching, or fuzzy logic, it is something else. It tests to see if memory corruption bugs are actually \textit{exploitable}, that is to say we can wrangle it for nefarious uses. This matters because the number of vulnerabilities per year is going up. This means that the system itself is less secure.

The process is to run a program on many \textit{abnormal/malformed} inputs and look for unintended behaviour. An observable side effect is essential, and it should be scalable. The underlying assumption is that if the unintended behaviour is dependent on an input, an attacker can craft such an input to exploit a bug.

There are multiple types of fuzzing:
\begin{itemize}
    \item Input based: mutational and generative
    \begin{itemize}
        \item Mutate seed inputs to create new test inputs
        \begin{itemize}
            \item Simple strategy is to randomly choose an offset and change the byte
            \item Pros: Very easy to implement and low overhead
            \item Cons: Highly structured inputs will become invalid quickly because it has low coverage.
        \end{itemize}
        \item Generation Based: Learn/create the format/model of the input and based on the learned model, generate new inputs
        \begin{itemize}
            \item Well-known file formats
            \item Pros: Highly effective for complex structured input parsing applications because it has high coverage
            \item Cons: Expensive as models are not easy to learn or obtain
        \end{itemize}
    \end{itemize}
    \item Application based: black and white box
    \begin{itemize}
        \item Black box: Only the interface is known
        \item White box: Inner workings of the program are known. Application can be analysed and monitored (can use static and dynamic analysis). Normally means we have the source code
    \end{itemize}
    \item Input Strategy: memory-less and evolutionary
\end{itemize}

The problem with traditional fuzzing is that if we use black-box and mutation, we are essentially aiming with luck. Think shit and walls. So, if we apply more heuristics to mutate better and learn good inputs, we can apply more analysis to understand the application behaviour. This is known as \textit{smart/evolutionary fuzzing}:
\begin{itemize}
    \item Recall: memory-less and evolutionary fuzzing
    \item Rather than just throwing inputs at the program, \textit{evolve them}
    \item The underlying assumption is that inputs are parsed enough before going further deep in execution
\end{itemize}

This will take some time and resources, so the scalability may be affected; do we have access to the resources and the required time to get this.

The feedback to enable the fuzzing to evolve should be either
\begin{itemize}
    \item Code-coverage based fuzzing
    \begin{itemize}
        \item Most of the contemporary fuzzers are here
        \item Uses code-coverage as the proxy metric for the effectiveness of a fuzzer
    \end{itemize}
    \item Directed fuzzing
    \begin{itemize}
        \item This method is not much explored
        \item The idea is that there should be a way to find the destination and a sense of direction.
    \end{itemize}
\end{itemize}

For smart code-coverage based fuzzers, it is important to have some knowledge about:
\begin{itemize}
    \item Where to apply mutation (which offsets in input)
    \item What values to replace with
    \item How to avoid traps (paths leading to error handling code)
\end{itemize}

\textbf{Symbex} can be used in combination with fuzzing and it means \textit{symbolic/concolic} execution. Unfortunately, Symbex is not very scalable because symbolic execution programs are very resource heavy. It does allow us to collect strings, however. We are able to enhance the scaling capabilities of symbex:
\begin{itemize}
    \item Native execution, contrary to IR based execution in existing symbex tools
    \item Instruction-level symbolic execution
    \item Optimistic solving
\end{itemize}

\textbf{VUzzer} goes even further with more analysis. The main idea is to prioritise/deprioritise certain paths, as some can be difficult to execute because they are guarded by constraints (nested conditions). They also leverage the application's control and data-flow features to infer input properties. It combines both static and dynamic analysis along with heuristics to improve coverage.

There are still problems, though. We are unaware of whether offset is processed by the application, which is a waste of mutation time. Also, we don't know what or where to mutate, since different bugs have different triggering conditions.

Evaluating fuzzers is also pretty hard, because how do we measure efficiency?
\begin{itemize}
    \item Could measure code-coverage, but what about directed fuzzers?
    \begin{itemize}
        \item Also for binary only fuzzers, measuring code coverage is not that straight forward.
        \item For Code based fuzzers, what about library code?
    \end{itemize}
    \item Uniqueness of crashes
    \begin{itemize}
        \item How do you differentiate between several crashes? Often, coredump does not have enough information
        \item Root cause analysis (not much is there)
    \end{itemize}
\end{itemize}

\subsubsection{Fuzzing for Race Bugs}
Applying fuzzing for data race bugs is really interesting (allegedly). Race conditions are pretty serious (we have looked at this before) so fuzzing for these is pretty important. Actually, RAZZER is a fuzzer that was used to find some new race condition bugs in Linux. 

We can fuzz the scheduler to identify these. Existing approaches are:
\begin{itemize}
    \item Identify shared objects
    \item An input that executes instructions involving shared objects
    \item Thread scheduler
    \begin{itemize}
        \item Rather than letting OS decide, introducing a scheduler that can control the thread scheduling
        \item Schedule threads with respect to different ordering
    \end{itemize} 
\end{itemize}

\section{Software Defence Mechanisms}
We will look at compiler and OS level protections. The \textbf{stack canary} (also called security cookies) are values added to binaries during compilation to avoid buffer overflow attacks. Another method is to use W\^X (write xor execute). ASLR (address space layout randomisation) is a technique that means it is harder to get solid memory addresses in the stack.

\subsection{Canaries}
We know from buffer overflow attacks that:
\begin{itemize}
    \item At CALL, return address is saved on the stack
    \item The stack grows downwards
    \item Local buffers are allocated on the stack
    \item Return address is POPed into the EIP
    \item EIP can point to anywhere in the memory
\end{itemize}

We can protect against this saved return attack by:
\begin{itemize}
    \item StackGuard protection
    \item Use a different prologue
    \begin{itemize}
        \item Push a canary into the stack
        \item It's a constant \texttt{0x000aff0d}
    \end{itemize}
    \item Different Epilogue
    \begin{itemize}
        \item Checks to see if the canary is still there. If it has been overwritten, we know that a buffer overflow has happened
    \end{itemize}
\end{itemize}

This works for \texttt{strcpy} because the \texttt{0x00} will stop \texttt{strcpy} from copying any further. These canaries are called \textit{terminator canaries}.

Unfortunately, this isn't perfect. Local variables that are located after (on top of) \textbf{buf} are not protected. Also, the saved frame pointer EBP can be altered.

StackShield is another protection method. It saves return addresses in an alternate memory space named \textbf{retarray}. Two gloabl variables are used: \textbf{rettop}, initialised on startup and is the address in memory where retarray ends. The other variable is \textbf{retptr} and is the address where the next clone is to be saved.

The function prologue ensures the return address is copied from the stack to \textbf{retarray} and \textbf{retptr} is incremented. The epilogue ensures the saved clone RET is retrieved and is checked with the RET from the stack. If it has changed, we can either overwrite the changed one, or just exit.

With newer versions of GCC, there is a new stack layout:
\begin{itemize}
    \item Function params
    \item Function return addresses
    \item Frame pointer
    \item Cookie
    \item Locally declared variables and buffers
    \item Callee save registers
\end{itemize}

\subsection{Writeable xor Executable}
The basic idea is that memory pages have permission to be either writeable or executable but not both. It protects against attacks that rely on code execution on data segments, and is a subclass of DEP (data execution protection).

\subsection{Address Space Layout Randomisation}
With ASLR, the predictability of memory addresses is reduced by randomising the address space layout for each instantiation of a program. Therefore, heap, stack, bss, and text section of the program get different addresses every time.

\subsection{Control Flow Integrity}
This is a strong attack mitigation technique. It restricts the control-flow of an application to \textit{valid} execution traces. The assumption is that any attack deviates from the intended execution (which is the set of precomputed or static states). At runtime, if an invalid state is detected, an alert is raised, usually terminating the application. It is not the same as bug detection because other runtime monitoring techniques like sanitisers target development setting, detecting violations when testing the program (for example, during fuzzing). CFI on the other hand is an active defence mechanism to mitigate an ongoing attack.

CFI detects control-flow hijacking attacks by limiting the targets of control-flow transfers, and consists of two abstract components:
\begin{itemize}
    \item Static analysis component that recovers the CFG of the application (at different levels of precision)
    \item The dynamic enforcement mechanism that restricts control flows according to the generated CFG. IT involves instrumentation (discussed earlier)
\end{itemize}

Control flow can be categorised in two ways:
\begin{itemize}
    \item Unconditional and conditional jumps
    \item Direct (target known at compile time) and indirect (target is known at runtime)
\end{itemize}

Due to the write integrity of the code section, direct transfers are protected, so it is the indirect control-flow that we want to protect.

\subsubsection{Types of indirect control flows}
\textbf{Forward edge transfers} direct code forward to a new location and are used in indirect jump and indirect call instructions (such as \texttt{jmp *rax} or \texttt{callq *rbx}). \textbf{Backwards-edge transfers} are used to return to a location that was used in a forward-edge earlier (such as returning from a function call through a return instruction). Therefore, we have both forward and backwards-edge CFI.

\subsubsection{Constructing CFG}
Depending on the precision, a combination of static and dynamic CFG construction can be used. Forward edge over-approximates the targets of the indirect transfer, while indirect function calls are complex:
\begin{itemize}
    \item Approximates based on function prototypes
    \item Different CFI mechanisms use different forms of type equality, so any valid function or functions with the same arity or functions with teh same signature. At runtime, any function with matching signature is allowed.
\end{itemize}

Function matching can be further enhanced by looking at \textbf{address-taken} functions. These are functions whose address is calculated and assigned. There are more complex approaches to enhance the precision such as path sensitive computations.

\subsubsection{Runtime enforcement}
For forward-edge transfers, the code is often instrumented with some form of equivalence check --- only valid targets are allowed. Each callsite and function entry-point is instrumented to check this equivalence (known as trampoline code). Backwards-edge transfers are harder to model as returns can come from any valid callsite. Shadow stacks are used to enforce the recent caller.

\section{Hardware Vulnerabilities}
We like to pretend that everything is digital and forget that hardware actually exists and can be a little bit analog. The bugs that we get from hardware can have some serious consequences for programmers.

\subsection{DRAM --- Rowhammer}
When we access memory, we activate the row in the bank by letting the capacitor discharge into an active memory buffer. We have to actively refresh the data stored in the capacitors. Unfortunately, electronic engineering is messy:
\begin{itemize}
    \item Capacitors are leaky!
    \item Current in wires can induce current in other nearby wires.
    \item 1s and 0s aren't really charged on uncharged capacitors, it's whether the capacitor is releasing more/less than a threshold voltage.
\end{itemize}

This used to be fine because electronic components were really large, but now that they're small, it's more of a problem. This leads to a bug in DRAM chips that has been known since 2010 called \textbf{Rowhammer}. The basic idea is that if you repeatedly charge and discharge a row in DRAM really quickly, it can sometimes cause errors in nearby rows. Manufacturers know about it, but it is not really documented since it is seen as a \textit{reliability} issue. Cached memory largely fixes this, too. It was also discovered that you can flip bits in memory without even accessing them.

Rowhammer looks like this (in assembly)
\begin{lstlisting}
code1a:
    mov (x), %eax
    mov (y), %ebx
    clflush (X)
    clflush (Y)
    mfence
    jmp code1a
\end{lstlisting}

Say X was in row 1 and Y in row 4, if you load X into the active buffer and Y into the b buffer really really fast, you'll get errors in rows 2 and 3. Some interesting things were discovered about this:
\begin{itemize}
    \item Bit flips are \textit{consistent}:
    \begin{itemize}
        \item You might not know which bit will flip, but it will be the same bit
        \item Same hardware generally flips the same bits
    \end{itemize}
    \item Double sided row hammering makes flipping bits \textit{really likely}
    \begin{itemize}
        \item Even in chips thought to be resistant
    \end{itemize}
\end{itemize}

If we can get a variable held in two different rows in one bank, we can induce a random, but consistent single bit error in surrounding rows, but it is hard to know how virtual memory translates to \textit{real memory locations}. So, there will be an element of getting lucky, although we only need to get lucky once, and we can increase this luck by running many things at once.

This is cool and all, but we're just producing single bit errors, so what is the use of this?

\subsubsection{NaCL}
NaCL is a sandbox for C/C++ code that aims to make things safe. The sandbox runs with privileges and checks whether a chunk of code is safe to run. If so, it loads into memory aligned on 32B boundaries. If we can corrupt this, we can run unsafe hidden code.

The useful thing about x86 architecture is that there is no requirement for aligning instructions. Different instructions have different lengths, and some have multiple lengths!

The code section is readable by loaded processes, so we can spot when the rowhammering has been successful.

Attack method:
\begin{itemize}
    \item Load a sequence of safe code that have unsafe instruction-sequences at 1-bit different offsets
    \item Rowhammer NaCL's code loading code (load NaCL into memory a lot and hope we get lucky)
    \item Either, we get an invalid instruction sequence (crash)
    \item Or we Rowhammer the kernels memory accidentally (crash again)
    \item Or we get lucky!
\end{itemize}

\subsubsection{Preventing Rowhammer}
We could buy better RAM, but how do you tell?

ECC RAM is an option that can fix single bit errors and reboot on 2 bit errors (but 3 bit errors are exploitable), but it is slower and more expensive.

\subsection{Speculative Execution}
Everyone remembers the fetch decode execute and write back cycle. This is how CPUs execute programs, and this cycle takes time (for each instruction, there are 4 things to do). On modern CPUs, we are able to parallelise (\textit{pipeline}) the whole process. As one instruction has finished fetching, the next is fetched while the first decodes. And then fetch the next as the first executes and the second decodes.

This works well so long as no instruction depends on the results of a prior one.

It becomes a little bit sticky after a conditional jump: do you know whether the program will take the branch? You could either block until you know for sure, or guess (and we guess).

If we correctly predict the branch, then great. If we get it wrong, this is fine \textit{as long as you catch it before writeback}. There are a bunch of algorithms for this, such as `always assume no', `backwards taken, forwards not', or `programmer hints'.

Modern branch prediction is much more complex, and a lot of the speed in computing comes from this prediction. The basic idea is that it keeps a record of what the branch has done before and predicts on the basis of that. It's not very well documented, varies chip by chip and is pretty obscure.

There are some unanswered questions here:  What happens if executing an instruction speculatively (based on branch prediction) causes data to be loaded into cache? If so, should you flush the cache on branch misprediction?

\subsection{Meltdown and Spectre}
This is a family of real world side channel attacks based on specualtive execution. Meltdown \textit{melts down} security barriers and spectre makes speculative execution scary. The attacks allow us to leak secure memory (in keys) and affects nearly all OSs and processor architectures. We do have some \textbf{software} mitigations available now, but they come at a considerable cost.

\subsubsection{Spectre}
Spectre really just looks like these two lines:
\begin{lstlisting}
if (x < array1_size)
    y = array2[array1[x] * 4096]
\end{lstlisting}

\texttt{array1} is a pointer to some region of memory. We are allowed to access \texttt{array1\_size} bytes of it. \texttt{x} is controlled by an attacker and 4096 is the size of a cache page.

Now, \texttt{if(x < array1\_size)} should fail, but it is going to take some time to get through the CPU pipeline. We can trick the branch predictor into guessing that the branch will be taken (make it succeed repeatedly beforehand) and then the second line will be executed speculatively.

\texttt{array2[array1[x] * 4096} is going to cause a page miss, so a new page will be loaded into the CPU's cache, and that page will depend on the value of \texttt{array1[x]} which would normally trigger a segmentation fault which happens during the last stage of a CPU's pipeline.

But, the exception won't be thrown because the branch predictor will catch up so the if statement should never have been taken, so all of the registers, exception flags, memory writes will be rolled back.

However, the CPU's cache is NOT flushed. So, the currently cached memory page is dependent on whatever was at that illegally accessed memory address. So, if you were able to time accessing each page of memory, you could find out which page was quick to access adn that would leak the value or \texttt{array1[x]}.

You can also trigger this with JS. So you could actually host a webpage on a shared server, dump all of memory for every process and user on there, and you can leak every key in memory.

\subsubsection{Mitigations}
Some software mitigations we can do are at the OS levels:
\begin{itemize}
    \item see \texttt{/sys/devices/system/cpu/vulnerabilities} on Linux
    \item Not perfect, but makes it \textbf{much} harder to exploit
\end{itemize}

But on an Intel CPU hyperthreading (SMT) makes exploiting \textbf{much} easier. Hyperthreading pretends that each core is essentially two cores (parallelisation is doubled). Hyperthreading should really be disabled, which is a massive shame because the performance cost of disabling this can be up to 25\%. 

\end{document}